{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import itertools\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "print(f\"Setup complete. Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NavigationGPT:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
        "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path, torch_dtype=torch.float32)\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 60):\n",
        "        encodings = self.tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(**encodings, max_new_tokens=max_new_tokens, num_beams=1)\n",
        "        return self.tokenizer.decode(output_ids[0, encodings.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "def generate_grid_graph(size=5):\n",
        "    G = nx.DiGraph()\n",
        "    num_nodes = size * size\n",
        "    nodes = list(set([''.join(random.choices(string.ascii_lowercase, k=2)) for _ in range(num_nodes*2)]))[:num_nodes]\n",
        "    node_map = [['' for _ in range(size)] for _ in range(size)]\n",
        "    for r in range(size):\n",
        "        for c in range(size):\n",
        "            idx = r * size + c\n",
        "            node_map[r][c] = nodes[idx]\n",
        "            if c < size - 1: G.add_edge(nodes[idx], nodes[idx+1], direction='EAST')\n",
        "            if c > 0: G.add_edge(nodes[idx], nodes[idx-1], direction='WEST')\n",
        "            if r < size - 1: G.add_edge(nodes[idx], nodes[idx+size], direction='SOUTH')\n",
        "            if r > 0: G.add_edge(nodes[idx], nodes[idx-size], direction='NORTH')\n",
        "    return G, nodes, node_map\n",
        "\n",
        "def generate_simple_random_walk(G, length):\n",
        "    start_node = random.choice(list(G.nodes()))\n",
        "    path = [start_node]\n",
        "    for _ in range(length - 1):\n",
        "        neighbors = list(G.successors(path[-1]))\n",
        "        valid_neighbors = [n for n in neighbors if n not in path]\n",
        "        if not valid_neighbors:\n",
        "            return None\n",
        "        path.append(random.choice(valid_neighbors))\n",
        "    return path\n",
        "\n",
        "def walk_to_string(walk, G):\n",
        "    if not walk or len(walk) < 2: return walk[0] if walk else \"\"\n",
        "    return \" \".join([f\"{walk[i]} {G.edges[walk[i], walk[i+1]]['direction']}\" for i in range(len(walk)-1)] + [walk[-1]])\n",
        "\n",
        "def get_node_coords(node_name, node_map):\n",
        "    for r, row in enumerate(node_map):\n",
        "        if node_name in row: return (r, row.index(node_name))\n",
        "    return None\n",
        "\n",
        "def get_manhattan_distance(n1, n2, node_map):\n",
        "    c1, c2 = get_node_coords(n1, node_map), get_node_coords(n2, node_map)\n",
        "    return abs(c1[0] - c2[0]) + abs(c1[1] - c2[1]) if c1 and c2 else float('inf')\n",
        "\n",
        "def create_test_cases(params):\n",
        "    test_cases = []\n",
        "    task_type = params.get(\"task_type\")\n",
        "    \n",
        "    pbar = tqdm(total=params['num_tests'], desc=f\"Generating tasks for {task_type}\")\n",
        "    attempts, max_total_attempts = 0, params['num_tests'] * 500\n",
        "\n",
        "    while len(test_cases) < params['num_tests'] and attempts < max_total_attempts:\n",
        "        attempts += 1\n",
        "        G, nodes, node_map = generate_grid_graph(params['grid_size'])\n",
        "        \n",
        "        if task_type == 'opposite_edge':\n",
        "            edge, size = [0, params['grid_size'] - 1], params['grid_size']\n",
        "            if random.random() > 0.5:\n",
        "                r = random.randint(0, size - 1)\n",
        "                start_node, end_node = node_map[r][edge[0]], node_map[r][edge[1]]\n",
        "            else:\n",
        "                c = random.randint(0, size - 1)\n",
        "                start_node, end_node = node_map[edge[0]][c], node_map[edge[1]][c]\n",
        "        elif task_type == 'high_manhattan_distance':\n",
        "            s, e = random.sample(nodes, 2)\n",
        "            if get_manhattan_distance(s, e, node_map) < params.get('min_md', 7):\n",
        "                continue\n",
        "            start_node, end_node = s, e\n",
        "        else:\n",
        "            logging.error(f\"Unknown task_type: {task_type}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            all_solution_paths = list(nx.all_shortest_paths(G, start_node, end_node))\n",
        "            if not all_solution_paths: continue\n",
        "            list_of_required_nodes = [set(p) for p in all_solution_paths]\n",
        "        except nx.NetworkXNoPath:\n",
        "            continue\n",
        "\n",
        "        context_found = False\n",
        "        for _ in range(250):\n",
        "            context = generate_simple_random_walk(G, params['context_walk_length'])\n",
        "            \n",
        "            if context is None: continue\n",
        "\n",
        "            nodes_in_walk = set(context)\n",
        "            if any(req.issubset(nodes_in_walk) for req in list_of_required_nodes):\n",
        "                context_found = True\n",
        "                break\n",
        "        \n",
        "        if not context_found:\n",
        "            continue\n",
        "\n",
        "        test_cases.append({\n",
        "            'graph': G, \n",
        "            'node_map': node_map, \n",
        "            'start': start_node, \n",
        "            'end': end_node, \n",
        "            'context': context\n",
        "        })\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    if len(test_cases) < params['num_tests']:\n",
        "        logging.warning(f\"Warning: Only generated {len(test_cases)}/{params['num_tests']} cases for {task_type}.\")\n",
        "    return test_cases\n",
        "\n",
        "def parse_path(text): return re.findall(r'\\b[a-z]{2}\\b', text)\n",
        "def is_valid_path(nodes, G): return all(G.has_edge(nodes[i], nodes[i+1]) for i in range(len(nodes)-1))\n",
        "\n",
        "def score_and_analyze(parsed_nodes, task):\n",
        "    try:\n",
        "        all_shortest_paths = list(nx.all_shortest_paths(task['graph'], task['start'], task['end']))\n",
        "        expected_len = len(all_shortest_paths[0])\n",
        "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
        "        all_shortest_paths, expected_len = [], -1\n",
        "    \n",
        "    is_path_valid = is_valid_path(parsed_nodes, task['graph'])\n",
        "    is_correct = (is_path_valid and parsed_nodes and all_shortest_paths and\n",
        "                  parsed_nodes[0] == task['start'] and parsed_nodes[-1] == task['end'] and\n",
        "                  parsed_nodes in all_shortest_paths)\n",
        "    \n",
        "    try:\n",
        "        path_dist_in_context = abs(task['context'].index(task['end']) - task['context'].index(task['start']))\n",
        "    except (ValueError, IndexError):\n",
        "        path_dist_in_context = float('inf')\n",
        "\n",
        "    return {\n",
        "        'accuracy': 1.0 if is_correct else 0.0,\n",
        "        'is_valid': is_path_valid,\n",
        "        'expected_len': expected_len,\n",
        "        'generated_len': len(parsed_nodes),\n",
        "        'manhattan_distance': get_manhattan_distance(task['start'], task['end'], task['node_map']),\n",
        "        'path_distance_in_context': path_dist_in_context\n",
        "    }\n",
        "\n",
        "class PromptStrategy(ABC):\n",
        "    @abstractmethod\n",
        "    def create_prompt(self, task: dict) -> str:\n",
        "        pass\n",
        "\n",
        "class StandardInstructionalStrategy(PromptStrategy):\n",
        "    def create_prompt(self, task: dict) -> str:\n",
        "        map_context_str = walk_to_string(task['context'], task['graph'])\n",
        "        instruction = f\"[SHORTEST] [START_NODE] {task['start']} [GOAL] {task['end']}\"\n",
        "        return f\"[SOS] {map_context_str} [SEP] {instruction} [PLAN]\"\n",
        "\n",
        "def run_advanced_analysis(model_config: dict, test_cases: list, model_name: str = \"model\"):\n",
        "    if not test_cases:\n",
        "        logging.warning(f\"Skipping evaluation for {model_name} as no test cases were provided.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        model = NavigationGPT(model_config['path'])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"SKIPPING {model_name}: Failed to load model. Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "    for task in tqdm(test_cases, desc=f\"Evaluating {model_name}\"):\n",
        "        prompt = model_config['strategy'].create_prompt(task)\n",
        "        generated_text = model.generate(prompt)\n",
        "        parsed_nodes = parse_path(generated_text)\n",
        "        analysis_metrics = score_and_analyze(parsed_nodes, task)\n",
        "        results.append(analysis_metrics)\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return pd.DataFrame(results)\n",
        "    \n",
        "class ExperimentLogger:\n",
        "    def __init__(self):\n",
        "        self._summary_results = []\n",
        "        self._detailed_results_list = []\n",
        "\n",
        "    def add(self, model_name: str, analysis_name: str, results_df: pd.DataFrame):\n",
        "        if results_df.empty:\n",
        "            logging.warning(f\"Skipped logging for {model_name}/{analysis_name} due to empty results.\")\n",
        "            return\n",
        "        accuracy = results_df['accuracy'].mean()\n",
        "        self._summary_results.append({\n",
        "            \"Model\": model_name, \"Analysis\": analysis_name, \"Accuracy\": accuracy\n",
        "        })\n",
        "        logging.info(f\"Logged: Model={model_name}, Analysis={analysis_name}, Accuracy={accuracy:.2%}\")\n",
        "        \n",
        "        detailed_df_copy = results_df.copy()\n",
        "        detailed_df_copy['Model'] = model_name\n",
        "        detailed_df_copy['Analysis'] = analysis_name\n",
        "        self._detailed_results_list.append(detailed_df_copy)\n",
        "\n",
        "    def get_summary_df(self) -> pd.DataFrame:\n",
        "        if not self._summary_results: return pd.DataFrame()\n",
        "        summary_df = pd.DataFrame(self._summary_results)\n",
        "        return summary_df.pivot_table(index='Model', columns='Analysis', values='Accuracy')\n",
        "\n",
        "    def get_detailed_df(self, analysis_name: str = None) -> pd.DataFrame:\n",
        "        if not self._detailed_results_list: return pd.DataFrame()\n",
        "        full_df = pd.concat(self._detailed_results_list, ignore_index=True)\n",
        "        return full_df[full_df['Analysis'] == analysis_name] if analysis_name else full_df\n",
        "\n",
        "def plot_stratified_results(combined_df: pd.DataFrame, analysis_name: str):\n",
        "    if combined_df.empty:\n",
        "        logging.warning(f\"Skipping plot for '{analysis_name}' due to empty data.\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "    fig.suptitle(f'Performance Analysis: {analysis_name}', fontsize=16)\n",
        "\n",
        "    sns.lineplot(data=combined_df, x='manhattan_distance', y='accuracy', hue='Model', errorbar='ci', ax=axes[0], marker='o')\n",
        "    axes[0].set_title('Accuracy vs. Path Difficulty (Manhattan Distance)')\n",
        "    axes[0].set_xlabel('Manhattan Distance between Start/End')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_ylim(-0.05, 1.05)\n",
        "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    df_filtered = combined_df[combined_df['path_distance_in_context'] != float('inf')]\n",
        "    if not df_filtered.empty:\n",
        "        sns.lineplot(data=df_filtered, x='path_distance_in_context', y='accuracy', hue='Model', errorbar='ci', ax=axes[1], marker='o')\n",
        "        axes[1].set_title('Accuracy vs. Contextual Proximity')\n",
        "        axes[1].set_xlabel('Start-End Distance in Context Path')\n",
        "    else:\n",
        "        axes[1].set_title('No Valid In-Context Paths Found')\n",
        "        \n",
        "    axes[1].set_ylabel('')\n",
        "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
        "    axes[1].legend().set_visible(False)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "print(\"Core functions and classes loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS_TO_ANALYZE = {\n",
        "    \"SP-RW\": {\n",
        "        \"path\": \"/cs/student/projects1/aibh/2024/cbaumgar/MSC_THESIS/sv3_model_ft/checkpoint-163000\",\n",
        "        \"strategy\": StandardInstructionalStrategy()\n",
        "    },\n",
        "    \"SP-Hamiltonian\": {\n",
        "        \"path\": \"/cs/student/projects1/aibh/2024/cbaumgar/MSC_THESIS/sv2_model_fixed/save-checkpoint-46000\",\n",
        "        \"strategy\": StandardInstructionalStrategy()\n",
        "    },\n",
        "}\n",
        "\n",
        "ANALYSIS_PARAMS = {\n",
        "    \"Generalization_5x5_OppositeEdge\": {\n",
        "        \"num_tests\": 1000, \n",
        "        \"grid_size\": 5, \n",
        "        \"context_walk_length\": 16,\n",
        "        \"task_type\": \"opposite_edge\",\n",
        "    },\n",
        "    \"Generalization_5x5_HighMD\": {\n",
        "        \"num_tests\": 1000, \n",
        "        \"grid_size\": 5, \n",
        "        \"context_walk_length\": 16,\n",
        "        \"task_type\": \"high_manhattan_distance\",\n",
        "        \"min_md\": 7\n",
        "    }\n",
        "}\n",
        "\n",
        "logger = ExperimentLogger()\n",
        "print(\"Configuration loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for analysis_name, params in ANALYSIS_PARAMS.items():\n",
        "    print(f\"\\n{'='*80}\\nGenerating shared test cases for '{analysis_name}'\\n{'='*80}\")\n",
        "    shared_test_cases = create_test_cases(params)\n",
        "\n",
        "    if not shared_test_cases:\n",
        "        logging.warning(f\"Skipping analysis '{analysis_name}' as no test cases could be generated.\")\n",
        "        continue\n",
        "\n",
        "    for model_name, config in MODELS_TO_ANALYZE.items():\n",
        "        print(f\"\\n--- Running model: {model_name.upper()} on {analysis_name} ---\")\n",
        "        \n",
        "        results_df = run_advanced_analysis(config, shared_test_cases, model_name)\n",
        "        logger.add(model_name, analysis_name, results_df)\n",
        "\n",
        "    combined_df_for_plotting = logger.get_detailed_df(analysis_name=analysis_name)\n",
        "    if not combined_df_for_plotting.empty:\n",
        "        plot_stratified_results(combined_df_for_plotting, analysis_name)\n",
        "\n",
        "print(\"\\n\\nAll analyses complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n\" + \"=\"*80 + \"\\n\" + \" \" * 20 + \"FINAL GENERALIZATION PERFORMANCE SUMMARY\" + \"\\n\" + \"=\"*80)\n",
        "\n",
        "summary_df = logger.get_summary_df()\n",
        "\n",
        "if summary_df.empty:\n",
        "    print(\"No analysis results to summarize.\")\n",
        "else:\n",
        "    print(\"\\n--- Summary Table ---\")\n",
        "    styled_table = summary_df.style.format(\"{:.2%}\", na_rep=\"-\").background_gradient(\n",
        "        cmap='viridis', vmin=0, vmax=1\n",
        "    ).set_caption(\"Model Generalization Performance Summary\")\n",
        "    \n",
        "    from IPython.display import display\n",
        "    display(styled_table)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
