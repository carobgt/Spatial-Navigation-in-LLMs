\chapter{Shortest Path Models}

The previous chapter demonstrated that a transformer can develop a robust, three-stage spatial reasoning algorithm from passive, exploratory learning alone. The Foraging Model successfully built a map-like representation of its environment simply by predicting the next step in a random walk. This chapter now asks a critical question: what happens to this learned algorithm when the model is subjected to an active, goal-directed planning objective? 

We now shift from passive exploration to active exploitation. The Shortest Path (SP) models are not trained on the purely local objective of predicting a valid next step, but on the global planning task of finding the most efficient route between two points. This imposes a strong optimisation pressure. Does this pressure force the model to develop a more sophisticated planning algorithm that leverages the underlying coordinate map? Or, does it encourage the model to abandon general map-building in favour of brittle, task-specific heuristics, effectively learning shortcuts instead of the map itself? To dissect this question, we analyse two variants of SP models. The SP-Hamiltonian (SP-H) model tests whether the agent can build and use a complete world model when given a structured, complete view of the environment: a direct test of its optimal planning capabilities. In contrast, the SP-Random Walk (SP-RW) model creates a more direct comparison to our baseline, forcing the agent to plan using the same kind of sparse, partial information the Foraging Model was trained on.

By applying the same behavioural, representational, and mechanistic toolkit used in the previous chapter, we can perform a direct, comparative analysis of the learned algorithms. This chapter investigates whether the introduction of a planning objective fundamentally alters the emergence of spatial intelligence, providing crucial insights into how different training paradigms shape the computational strategies learned by transformers.


\section{Behavioural Analysis}

We evaluate the SP models' spatial reasoning capabilities using the evaluation framework described in Chapter 3, assessing core performance metrics, context length robustness, and generalisation capabilities. The analysis reveals distinct behavioural patterns that contrast sharply with the Foraging Model's robust generalisation.

% \begin{table}[h]
% \centering
% \caption{Performance Comparison: SP Models vs Foraging Model}
% \label{tab:sp_performance_comparison}
% \begin{tabular}{lccc}
% \toprule
% Model & Context Type & Accuracy & Context Length \\
% \midrule
% SP-H & Both & 0\% & $\neq16$ steps \\
% SP-H & Hamiltonian & 100\% & 16 steps \\
% SP-RW & Hamiltonian & 92.5\% & 16 steps \\
% SP-RW & Random Walk & 99.5\% & 10 steps \\
% SP-RW & Random Walk & 97\% & 50 steps \\
% SP-RW & Random Walk & 13.5\% & 55 steps \\
% Foraging & Random Walk & 98.3\% & 120 steps \\
% \bottomrule
% \end{tabular}
% \end{table}

\subsection{Core Performance and Context Robustness}

The SP-H model achieves perfect accuracy on its training distribution: 4×4 grids with Hamiltonian context walks of exactly 16 steps, regardless of path complexity (Manhattan distance between start/goal nodes, as seen in Fig. ~\ref{fig:sp_manhattan_distance}A). This performance extends to a hold-out test set of unseen Hamiltonian `shapes', suggesting that the model has learned to solve the specific task rather than memorising individual examples. However, this success comes at a cost: the model fails catastrophically when context length deviates from 16 steps, even by a single step. To test whether this brittleness stems from the content of the context (unstructured vs. structured) or merely its length, we evaluated the SP-H model on 16-step random walks, providing it with a context that is the same length as its training data but lacks the ordered, non-repeating structure of a Hamiltonian path. Critically, we guaranteed task solvability: each random walk was generated to contain all necessary nodes for at least one valid shortest path between randomly selected start/goal nodes. On this task, its performance was limited, with 36.5\% accuracy on average, dropping with Manhattan distance (MD) (Fig. ~\ref{fig:sp_manhattan_distance}A).

The SP-RW model, finetuned from SP-H on variable-length random walk contexts (10--50 steps), demonstrates more flexible performance. It maintains high accuracy across the full range of its training context lengths (99.5\% at 10 steps to 97\% at 50 steps). When tested just beyond its finetuning window at a context length of 55 steps, performance drops to 13.5\%. This is a classic example of a transformer's failure to extrapolate beyond its training context window, a limitation also observed in the Foraging Model at its 120-step limit. Nevertheless, the SP-RW model achieves 99\% accuracy on the original Hamiltonian context task, indicating that it retains prior knowledge while improving flexibility during the fine-tuning process. It also exhibits somewhat consistent performance for both random walk and hamiltonian contexts on a 4x4 grid, regardless of path complexity (Fig. ~\ref{fig:sp_manhattan_distance}A).


\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/spmodelperformances.png}
\caption[Performance across Manhattan Distances on 4×4 and 5×5 grids.]{\textbf{Performance across Manhattan Distances on 4×4 and 5×5 grids.} (A) Manhattan Distance between Start/Goal on 4×4 grids showing SP-H (purple) and SP-RW (green) performance across different context types. (B) Manhattan Distance between Start/End on 5×5 grids showing SP-Hamiltonian (blue) and SP-RW (green) generalisation performance. SP-RW shows gradual decline from 95\% at MD 1 to 22\% at MD 8, while SP-Hamiltonian maintains reasonable performance only for very short paths (83\% at MD 1) before rapidly degrading to 0\% by MD 7.}
\label{fig:sp_manhattan_distance}
\end{figure}


\subsection{Generalisation Performance}

%TODO: add CL robustness plot to appendix and mention here

\subsubsection{High Manhattan Distance Tasks}

A critical challenge emerges when evaluating SP-H on larger grids due to its sensitivity to context structure. Unlike SP-RW, which was trained on variable-length random walks, SP-H was trained exclusively on 16-step Hamiltonian paths where each node appears exactly once. On a 5×5 grid containing 25 nodes, generating a traditional 16-step Hamiltonian path becomes impossible, creating an unfair evaluation scenario. To enable fair comparison, we adapt the evaluation protocol for SP-H by generating simple random walks (non-repeating paths) of length 16 on 5×5 grids. These context walks are constrained to be solvable as mentioned previously. This ensures that the model has sufficient spatial information to solve the task, while maintaining the non-repeating structure it was trained on.

The results reveal notably different generalisation capabilities between the models (Fig. ~\ref{fig:sp_manhattan_distance}B). SP-RW exhibits a stable performance degradation, starting at 90.4\% accuracy on MD 1 tasks and showing a gradual, nearly linear decline to 21.4\% at MD 8. This reasonable performance beyond MD 6 (the maximum possible distance on its 4×4 training grid), shows meaningful transfer of spatial reasoning beyond training constraints. In contrast, SP-H exhibits a much steeper exponential decay pattern. Starting from 82\% accuracy on MD 1 tasks, performance drops sharply to 0\% by MD 7-8. Hence, it seems that the SP-H model's reasoning ceiling is MD 6.

\subsubsection{Edge-to-Edge Tasks}
Having observed that SP-H cannot generalise beyond its training reasoning ceiling of MD 6, a new question arose: is its poor generalisation due to the path length exceeding its training data (MD $\leq 6$), or is it a more fundamental inability to apply its reasoning to a novel grid layout? To disentangle these factors, we designed the edge-to-edge navigation task (see Chapter 3 for details). Although edge-to-edge paths cannot be directly mapped to any configuration within a 4×4 training grid (e.g., four steps east), they require planning only 4 steps ahead: well within the MD 6 boundary that defines the model's training constraints. This makes edge-to-edge the most basic test of whether a model can apply its learned spatial reasoning to novel spatial configurations that remain within its established planning horizon. SP-RW achieves 78\% accuracy, while SP-H achieves only 3.6\%. The high performance of SP-RW on edge-to-edge tasks suggests that it has learned robust directional reasoning that can be extended to novel contexts. In contrast, SP-H's near-complete failure is particularly revealing: despite having the computational capacity to plan 4 steps ahead (Fig. \ref{fig:sp_manhattan_distance}A), it cannot generalise this capability to the novel spatial context of an edge-to-edge path on a larger grid. Thus, it is clear that fine-tuning on suboptimal walks not only increased the model's generalisation to new context lengths, which is perhaps trivial and expected, but also its ability to adapt to larger grids and extend its planning horizon.


\section{Representational Analysis}

Next, we examine the internal representations of the SP models using the same analytical framework as the Foraging Model, with certain methodological differences. Firstly the choice to average across node occurrences is context dependent. Unlike the Foraging Model's analysis, which averaged node representations across many random walks, here we must be more nuanced. Hamiltonian contexts contain each node only once, so averaging is not possible. For SP-RW, we will analyse both averaged and non-averaged representations to draw the clearest comparisons. Second, PCA can be conducted on either context or task nodes. Our goal is to understand how the pressure of a goal-directed planning objective alters the map-building process we observed in the Foraging Model, so we focus on context nodes here.

Thus, we stratify our analysis by context type (random walk contexts for SP-RW, Hamiltonian contexts for both models), and context length for random walks (10--50 steps). 

% TODO: add this for SPH symmetry collapse: However, we note that PCA is a linear dimensionality reduction technique, and the models' representations may lie on non-linear manifolds that are not fully captured by this analysis.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/sprwvsf.png}
\caption[PCA comparison between SP-RW and Foraging models.]{\textbf{PCA comparison between SP-RW and Foraging models.} Top row: Foraging Model showing three-stage evolution from noisy coordinates (Layer 1) to clean coordinate system (Layer 7) to functional clustering by navigational affordances (Layer 12). Bottom row: SP-RW model showing gradual refinement (Layers 1-7) followed by representational collapse into compressed vertical columns (Layer 12). Both models start similarly but develop fundamentally different representational strategies based on their training objectives.}
\label{fig:sp_pca_comparison}
\end{figure}

\subsection{Foraging Model vs. SP-Random Walk}

To create a direct, apples-to-apples comparison with the Foraging Model, we first analyse the SP-RW model's representations on 3x3 grids using the same methodology: averaging node representations across 500 random walks. The results reveal a shift in the nature of the learned spatial representations, despite shared architectural constraints and the processing of identical walks (Figure~\ref{fig:sp_pca_comparison}). The Foraging Model, as seen in Chapter 4, develops a clean world model. Its representations evolve from a noisy sense of coordinates in Layer 1 to a near-perfect Cartesian grid in Layer 7, suggesting the formation of a stable, path-independent coordinate system. The later layers refine this map to emphasise navigational affordances. Interestingly, the SP-RW model's first-layer representations are nearly identical to the Foraging Model's, despite its pre-training on completely different, structured Hamiltonian paths. This shared starting point quickly diverges. By Layer 7, the same pattern is still present but slightly sheared for SP-RW, lacking the clean structure of the Foraging Model. By Layer 12, its representations have collapsed into tight, compressed columns. The two models begin with the same raw spatial sense, but develop fundamentally different representational strategies based on their training objectives.

\subsection{SP-Random Walk}

When analysing the SP-RW model’s hidden states without averaging across node occurrences, we retain the contextual information associated with each node; specifically its arrival direction and its position within the path. This leads to a very different representational geometry compared to averaged analyses. Instead of forming a single stable map of coordinates, the embeddings are strongly trajectory-dependent. We observed that nodes cluster primarily according to their arrival direction (Figure \ref{fig:29th_node_effect}, left). For instance, the same coordinate, such as (0,0), does not map to a single point but appears in both the `SOUTH' and `EAST' clusters, depending on the preceding move. This phenomenon is intuitive: the direction from which a node is entered constrains the set of available next moves and thus provides a useful proxy for local navigational constraints. Start nodes, which lack an arrival direction, form their own distinct cluster. Importantly, this trajectory-dependent clustering is not unique to SP-RW; the same pattern is observed when the Foraging model is examined without averaging (Figure \ref{fig:29th_node_effect}, right).

Across both RW and Hamiltonian contexts, variation with respect to path index is consistently present in the embeddings. However, this should not be over-interpreted as evidence that the models explicitly encode sequence position for spatial reasoning. Two alternative explanations are plausible. First, the effect likely reflects the influence of positional embeddings, which are a standard component of transformer architectures and therefore expected in any model. Second, the pattern may act as an implicit uncertainty signal: nodes appearing earlier in the sequence, when less contextual information is available, tend to cluster together, while later nodes spread out as more structural constraints accumulate. Crucially, this path index variance is also not unique to the SP models. The Foraging model exhibits the same behaviour, but the effect is masked when averaging over node occurrences.
%TODO: add late context performance tasks to appendix and mention here
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/29thnode.png}
\caption[The 29th Node Effect in SP-RW compared to Foraging Model.]{\textbf{The 29th Node Effect in SP-RW compared to Foraging Model.} Left panel shows SP-RW model PCA representations at different context lengths (CL 28, 29, and 40) for a 3×3 grid, with all points colored by arrival direction. At CL 28, nodes maintain clean arrival direction clustering. At CL 29 (transition point), some nodes begin to be misclassified—note the example node (red box) that arrives from NORTH but appears in the WEST cluster. At CL 40, nodes collapse into four entangled clusters with no clear organizing principle. Right panel shows the Foraging model at CL 50 (unaveraged) for comparison, demonstrating consistent arrival direction clustering across all context lengths with no such transition or confounding effects.}
\label{fig:29th_node_effect}
\end{figure}
An interesting representational transition occurs at the 29th node of a given path: nodes with path indices below 29 maintain the clean arrival direction clustering pattern, while nodes with path indices above 29 collapse into four entangled clusters with no obvious organisational principle. The mechanism behind this transition remains unclear, a plausible explanation is the training data distribution: random walks were sampled with lengths between 10 and 50, with a midpoint around 30, potentially inducing a change in representational strategy near that range. However, other explanations are possible, and this represents an area for future investigation. Importantly, this representational change does not impair task performance—specialized behavioural tests show that the model maintains high accuracy (93-97\%) on tasks requiring information from the `late' portion of context walks, demonstrating that the transition represents a change in representational strategy rather than a functional limitation.

When tested on Hamiltonian contexts, the SP-RW model shows similar patterns to random walk contexts, with arrival direction being the primary organisational principle. We observe sub-clustering based on path index as expected.


\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/combinedsph.png}
\caption[Horizontal Mirroring Effect in SP-Hamiltonian Model.]{\textbf{Horizontal Mirroring Effect in SP-Hamiltonian Model.} Left panel shows PCA representations for layers 1 and 2 (columns) with three different coloring schemes (rows): arrival direction (AD), coordinates, and path index. The horizontal mirroring pattern is evident across all layers and coloring schemes. Right panel visualizes the specific coordinate pairs that demonstrate perfect horizontal mirroring: coordinates (0,0) and (0,2) appear nearly identical in PCA space, as do coordinates (2,0) and (2,2), illustrating the model's symmetric representation across the central horizontal axis of the 4×4 grid. This mirroring effect explains why the model shows overlapping north/south directions while maintaining distinct east/west clusters.}
\label{fig:sph_horizontal_mirroring}
\end{figure}


\subsection{SP-Hamiltonian}

The PCA analysis reveals a suprisingly different representational pattern in SP-H: the model shows strong horizontal mirroring in its context node representations. Coordinates (0,0) and (3,0) appear nearly identical in the PCA space, as do (0,3) and (3,3), with similar patterns for all coordinate pairs across the central horizontal axis. This pattern is observed for all coordinate pairs: (0,1) with (3,1), (0,2) with (3,2), (1,0) with (2,0), (1,1) with (2,1), (1,2) with (2,2), and (1,3) with (2,3).

This perfect horizontal mirroring across the central horizontal axis of the 4×4 grid could explain why north/south directions are seem to overlap, whereas east/west show more distinct clusters. However, we note that PCA is a linear dimensionality reduction technique, and the models' representations may lie on non-linear manifolds not fully captured by this analysis. The observed patterns represent projections of potentially more complex high-dimensional structures onto two-dimensional spaces (three principal components here), which would explain the apparent symmetrical collapse. Interestingly, this pattern is completely different from the Hamiltonian context node representations of SP-RW, despite it being fine-tuned from SP-H. In contrast, SP-RW exhibits nearly identical PCA patterns for both Hamiltonian and random walk contexts. This consistency makes sense: a Hamiltonian path can be viewed as a subset of a random walk, but not vice versa. During fine-tuning from SP-H, SP-RW appears to have adapted its representations to support both context types simultaneously, rather than switching strategies depending on the context. 


\subsection{Layer-Wise Dynamics}

For both SP-RW and SP-H, cluster structure is remarkably stable across layers. The cluster shapes evolve gradually, suggesting iterative refinement of representations. This contrasts with the Foraging Model, which exhibited a sharp representational shift at Layer 7—a qualitative reorganisation of clusters. This indicates that SP models may rely more on incremental spatial reasoning.


\section{Mechanistic Analysis}

The behavioural and representational analyses reveal clear differences between the SP models, but the underlying computational mechanisms remain partially understood. The SP models' mechanistic analysis is more limited than what we might achieve with more extensive experiments. However, the available evidence provides insights into why finetuning on random walks led to increased robustness.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{Latex template/figures/DAsphcontexts.png}
\caption[Direction token ablation results for SP models.]{\textbf{Direction token ablation results for SP models.} Comparison of task accuracy when historical direction tokens are zeroed out at the input to each layer. Both the SP-H (blue) and SP-RW (brown) models show a gradual recovery, demonstrating a continuous reliance on direction tokens throughout the network.}
\label{fig:sp_direction_ablation}
\end{figure}
\subsection{Direction Token Ablation: Continuous Dependence}

Layer-wise direction ablation experiments reveal a key algorithmic distinction between the exploratory Foraging Model and the goal-directed SP models. In the Foraging Model, ablating direction tokens revealed a sudden transition at Layer 7, where its internal spatial map became self-sufficient. Based on our SP models' PCA analyses, which showed stable internal representations across layers, we hypothesised that the SP models would process directional information through a fundamentally different mechanism. The ablation results confirm this hypothesis. Both SP-RW and SP-H models exhibit continuous dependence on directional information throughout all network layers, with a gradual, monotonic increase in accuracy as the ablation layer increases (Figure \ref{fig:sp_direction_ablation}). There is no transition point where performance suddenly recovers. Instead, accuracy is near 0\% when ablating early layers and smoothly approaches baseline performance only when layers 11 and 12 are spared. Interestingly, for SP-H, it never quite recovers its perfect accuracy. This provides strong causal evidence that, unlike the Foraging Model, the SP models never consolidate spatial information into a self-sufficient map. Their algorithm is one of continuous, path-dependent computation, where explicit directional tokens remain critical throughout all 12 layers. This gradual processing is consistent with the PCA results, which showed iterative representational refinement across layers rather than the drastic representational rearrangement seen in the Foraging Model.

A surprising result, however, was the near-identical ablation curves for SP-H and SP-RW. Despite SP-RW's superior robustness and different representational geometry, its core reliance on direction tokens is mechanistically identical to SP-H's. This finding highlights an important distinction between representational similarity and algorithmic similarity. Although SP-RW develops representations that are more similar to the robust Foraging model (as evidenced by PCA), it nonetheless seems to employs the same continuous, path-dependent computational strategy as SP-H. This interpretation aligns with established findings in the literature: rather than fundamentally altering learned algorithms, fine-tuning typically teaches existing computational mechanisms to tolerate a wider variety of inputs while preserving core processing strategies. The robustness improvement appears to stem from better representational organisation rather than a fundamental algorithmic change—the existing algorithm simply learned to work with more Foraging-like representations while maintaining the same mechanistic dependencies on directional information. 

\subsubsection{Representational vs. Functional Symmetry}

The SP-H model's PCA analysis revealed a pattern in context node representations, where coordinates symmetric about the central horizontal axis overlap in the embedding space. This pattern might suggest asymmetric directional processing—perhaps the model distinguishes east-west movements while conflating north-south directions. However, targeted ablation experiments contradict this interpretation. Removing either NORTH/SOUTH tokens or EAST/WEST tokens produced identical performance drops, demonstrating that the model treats all four cardinal directions equivalently during computation. This indicates that the SP-H model's functional use of directional information is globally symmetric, even if PCA shows subtle separations along one axis. In other words, while PCA captures representational geometry, it does not fully reveal the model's computational reliance on directional cues. Taken together, these results demonstrate that SP-H embeddings encode positional information in a highly symmetric fashion, which is likely due to the symmetric structured nature of Hamiltonian paths. This likely contributes to the model's steep decay in generalisation and explains why fine-tuning on unstructured walks mitigated this. 

% TODO: need to nuance claims more, 'Foraging learns map but SP doesnt' is a brutal oversimplification. Its more likely that exploratory random walk training/finetuning is actually what encouraged the map-like representation, since we see similar PCA patterns between SPRW and Foraging, but totally different ones in SPH. This explains their robustness to grid sizes and reasoning ceiling. However, this comes at the cost of specialisation: finetuning on RWs led to a drop in ID performance, only SPH achieved 100\% accuracy.
% also note that SPRW is probably not as clean as Foraging, has lower acc and also weird 29th node pattern which may represent an imperfect process or limitation of finetuning--> model couldnt perfectly implement foraging models learned reps
% direction ablation seems to contrast with PCA and behavioural results--> if robust and similar to foraging why different? perhaps SPRW learns representations that are more 'map-like', similar to foraging model, but that the way it uses these representations fundamentally differs: continuous reliance on direction tokens vs foraging model transition at layer 7. There is no clear switch to 'map-using'. Again this is probably an oversimplification since the PCA results are qualitative and correlational.
% TODO need to double check foraging pca, clusters by AD only for layer 1--> pca is pretty unreliable, but direction ablation results are strong

\section{Discussion}

The previous chapter demonstrated that transformers can develop a robust, map-like representation of space through passive, exploratory learning. In this chapter, we investigated what happens when the training objective shifts from passive exploration to active, goal-directed planning. Our primary research question was whether transformers learn universal `cognitive maps' or task-specific heuristics. The answer is not binary; rather, models lie along a spectrum determined by the interaction between their training objective and the statistical structure of their data. This analysis suggests a trade-off: models can develop robust, generalisable spatial reasoning, but doing so may come at the expense of highly optimised, task-specific performance. 

\subsection{Mechanistic Comparison}
This high-level strategic difference is causally substantiated by our mechanistic analyses, which reveal two distinct computational algorithms. The Foraging Model's direction ablation experiments demonstrated a clear phase transition at Layer 7, the point at which its internal coordinate map becomes causally self-sufficient and no longer relies on explicit historical direction tokens. This suggests a process of information consolidation, where local movement cues are integrated into a stable, abstract representation. The SP models show no such transition. For both SP-H and SP-RW, performance recovers gradually and monotonically as direction ablation is moved to later layers, providing strong evidence that their algorithm remains continuously dependent on explicit direction tokens throughout all 12 layers. There is no sharp transition point where the model becomes self-sufficient. Instead, their reasoning appears to be an ongoing, step-by-step calculation tethered to the sequence of moves provided in the context.

This gradual recovery has important nuances. The SP-H model, for instance, never fully recovers its perfect baseline accuracy, even when directional information is only ablated in the final layer. This suggests its highly specialised algorithm is extremely sensitive, requiring an uninterrupted flow of directional cues through the entire network to execute its procedure flawlessly. The case of the SP-RW model is more ambiguous. On one hand, it recovers to its baseline accuracy by layer 10. Given its training on random walks and its representational similarities to the Foraging Model, this \textit{might} suggest that it has an internal map which becomes sufficiently stable by this point. However, we cannot be certain. The recovery is gradual, lacking the sharp phase transition that characterises the Foraging Model's shift to map-based computation. Furthermore, because SP-RW’s baseline accuracy is lower ($\approx92\%$), small deviations in performance are less diagnostic than for the Foraging Model or SP-H, where any drop from 100\% reflects a clear mechanistic disruption.


\subsection{Representational Comparison}
This mechanistic difference provides a clear lens through which to interpret the representational findings. The Foraging Model’s three-stage evolution—from noisy coordinates to a perfect Cartesian grid to functional affordance clusters—indicates a representational hierarchy that mirrors the consolidation of a stable, allocentric map. This structure allows the model to support flexible generalisation, as its later layers encode not just where things are, but what can be done from those positions. Crucially, this is consistent with its mechanistic transition at Layer 7, where local directional inputs cease to be necessary, and the learned map becomes causally self-sufficient. In contrast, the SP models show no such phase transition, and their representational footprints reflect this. The perfect horizontal mirroring in the SP-H model's representations is a particularly telling example. This symmetry is likely not an error but a clever, albeit brittle, feature learned from the highly structured and symmetric nature of Hamiltonian path data. The model discovered a compression strategy that exploits the regularities in its training distribution, but this shortcut breaks down on the unstructured, asymmetric data of random walks or larger grids. 

The SP-RW model occupies a middle ground. Its representations begin similarly to the Foraging Model’s, suggesting that fine-tuning on random walks reintroduces pressures toward allocentric map-like organisation. However, instead of converging to a clean grid, they shear, and ultimately collapse into compressed columns, likely reflecting some useful structure that is uninterpretable with linear dimensionality reduction. After position index 29, nodes are further organised into four coarse clusters with no apparent organising principle. Behaviourally, this collapse does not translate into failure, since the model remains accurate on tasks that depend on late-context information. What it does indicate is that the representational strategy of SP-RW is less cleanly defined than that of the Foraging Model. Whether this reflects the limited strength of fine-tuning (competing pressures of Hamiltonian pre-training and random walk fine-tuning may indicate that the model has not converged to a stable spatial encoding), or a consequence of the goal-directed training objective leading to less interpretable representational structure, remains open.


\subsection{The Effect of Fine-tuning}

The relationship between the SP-H and SP-RW models offers a nuanced insight into the mechanics of fine-tuning. Fine-tuning on random walks dramatically improved SP-RW’s generalisation, allowing it to handle varied context lengths, larger grids, and longer planning horizons. One might assume this improvement came from learning a new, more robust algorithm. However, our causal analysis suggests otherwise. The direction ablation experiments reveal that both models share a similar continuous dependence on direction tokens, pointing to a computational mechanism of path-dependent reasoning, rather than a shift towards the map-based strategy seen in the Foraging Model. Representational analysis offers further insight. Node embeddings show that SP-RW and the Foraging Model share similar patterns: collective embeddings cluster by arrival direction across all layers, and averaged node representations are nearly identical at Layer 7, though the Foraging Model undergoes a subsequent shift. The source of SP-RW's improved robustness, therefore, appears to be representational rather than algorithmic. Note that a sufficiently large representational change could, in principle, be considered an `algorithmic change' in its own right. Here, we define an `algorithmic change' as a fundamental shift in the causal flow of information through the network, while `representational change' refers to modifications in how information is encoded without altering the underlying computational mechanism. 

Rather than inducing a new algorithm, the fine-tuning process appears to have taught its existing path-dependent algorithm to tolerate a wider variety of inputs. By being exposed to the messy, unstructured nature of random walks, the model was forced to develop representations (like the arrival-direction clustering) that were less brittle than the symmetric representations learned from Hamiltonian paths. This finding aligns with the literature, which notes that fine-tuning often adapts existing circuits to new data distributions rather than inducing the formation of entirely new algorithms [\cite{prakash2024finetuning}. This is also consistent with our observation of the loss curves during training for both models, where SP-H exhibited several `phase changes', but SP-RW did not. In the literature, phase transitions are abrupt, non-linear changes in a deep learning model's training loss over time, which usually signal the emergence of new behaviours or strategies within the model [\cite{chen2025suddendropsloss}]. We examine these phase transitions in more detail in Appendix \ref{fig:training_loss_curves}.


\subsection{Allocentric Maps vs Egocentric Path Integration}

Comparing the Foraging and SP models allows us to answer our primary research question directly. The core contrast is not simply that the Foraging Model learns a map and the SP models do not. Rather, the models learn different spatial reasoning strategies that lie on a spectrum from purely egocentric path-dependence to a more allocentric, map-based understanding. At one extreme lies the SP-H model. Trained exclusively on structured, optimal Hamiltonian paths, it develops a highly efficient but brittle algorithm. Its strategy seems to be akin to procedural path integration—continuously tracking movement from a start to a goal without forming a reusable, abstract model of the environment. This approach is highly effective for the narrow task it was trained on, enabling it to achieve perfect accuracy. However, this specialisation renders it fragile; its performance collapses when conditions deviate even slightly, as seen in its failure on non-standard context lengths or unstructured paths. At the other extreme, the Foraging Model, trained on unstructured random walks, is compelled to develop a different solution. To reliably predict the next step from any point in a meandering path, it must consolidate its experience into a coherent, path-independent world model. This results in a self-sufficient, allocentric representation of space—similar to a cognitive map. The SP-RW model sits somewhere in between. Its exposure to random walks during fine-tuning pushes its internal representations towards the allocentric, map-like end of the spectrum, and improved its generalisation capabilities. This is similar to the concept of \textit{latent learning} observed in mammals, where exploration without immediate reward supports the formation of a cognitive map that later facilitates goal-directed behaviour \citep{tolman1930maze,whittington2022howtobuild}, albeit in reverse in this case.
 %TODO: add latent learning in rats

\subsection{Limitations}
Several important questions remain unanswered. Our mechanistic analysis of the SP models, while revealing the core algorithmic difference in directional processing, is less complete than our analysis of the Foraging Model. We have established that the models rely on a continuous, path-dependent heuristic, but have not reverse-engineered the specific computations that perform path planning. Future work could use more granular patching experiments—for instance, patching the representations of start and goal tokens—to trace how the model selects and generates the optimal path. Furthermore, while our evidence suggests that SP-RW's robustness comes from a representational shift, this is still unclear. The precise nature of that shift and whether it enables the old algorithm to work on new data remains an open question for further investigation. We cannot definitively rule out alternative explanations for the robustness improvement. Finally, as noted in the previous chapter, our analysis is also limited to a specific architecture and environment (4×4 grids). It remains unclear how these patterns would extend to larger models or more complex environments. The bounded generalisation observed in the SP models might be overcome with different architectures or training procedures.

\subsection{Broader Implications}
Taken together, our comparative analysis offers insights into how training objectives and data characteristics shape the emergence of intelligence in transformers. The passive, exploratory objective of the Foraging Model created an inductive bias toward learning general, reusable structures—in this case, a cognitive map. Because it couldn't know what information would be relevant for predicting the next step in an arbitrary walk, its best strategy was to model the entire environment. Conversely, the active, goal-directed objective of the SP models created an inductive bias toward finding the most computationally efficient solution to a specific problem, leading to specialised, but more brittle heuristics. %TODO: need to add nuance here, how finetuning helped

This has implications for training AI systems for complex planning and reasoning. If a model is trained exclusively on optimal, expert demonstrations (akin to our Hamiltonian and shortest-path data), it may learn powerful but narrow strategies that fail to generalise to novel situations. In contrast, training paradigms that incorporate exploration, sub-optimal data, and a degree of randomness (like the Foraging Model's random walks) may be key for fostering the development of robust, generalisable world models. The apparent `inefficiency' of passive exploration may be a necessary catalyst for the kind of compositional learning that underpins true spatial intelligence. This work suggests that to build truly generalist agents, we may need to balance the pressures of optimisation with the creative uncertainty of exploration, ensuring our models learn the map, not just the shortcuts.