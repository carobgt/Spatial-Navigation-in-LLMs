\chapter{Conclusion}

The investigation of spatial reasoning in transformers has revealed how training objectives and data shape the computational strategies that emerge during learning. What began as a question about whether transformers learn cognitive maps or heuristics has evolved into an understanding of how different training frameworks produce distinct computational architectures. By systematically analysing three models trained on different spatial navigation tasks, we have revealed how training objectives shape not just performance, but the underlying computational strategies that emerge during learning. The work presented here suggests that the question of whether transformers learn universal cognitive maps or task-specific heuristics is not binary, but reveals a spectrum of spatial intelligence that emerges from the interaction between training objectives and data structure. These different forms of intelligence reflect distinct computational architectures that can be causally dissected and understood. The implications extend beyond spatial reasoning to how training paradigms shape intelligence emergence in neural networks.

\section{The Foraging Model's Adaptive Strategy}

Perhaps the most significant discovery was the Foraging Model's adaptive computational strategy. Rather than using a single approach, the model appears to employ qualitatively different algorithms depending on context length. With minimal context (2-3 steps), the model appears to rely on local heuristics, specifically a strong bias toward reversing the previous move: a conservative strategy that avoids invalid moves across all grid configurations. This heuristic-based approach achieves perfect accuracy because it exploits the geometric constraints of grid navigation without requiring global spatial understanding. Our mechanistic analysis provides a direct causal explanation for this behaviour. We found a consistent attention pattern in Layer 1 where multiple heads focus on the penultimate node when processing a direction. For a 2-step path, this circuit directly implements the reversal heuristic, solving the task in the earliest layers. Convergent evidence comes from our direction ablation experiments, where performance on 2-hop loops recovers to 100\% by Layer 2, confirming the existence of a specialised, low-level circuit for these trivial cases.

However, for longer paths, this local heuristic is insufficient. The model transitions to a more deliberative, map-based strategy. The critical transition occurs around Layer 7, where the model's internal coordinate system appears to become causally self-sufficient. The direction ablation experiments for complex loops (4-12 hops) reveals a sharp transition to 100\% accuracy between layers 6-8, suggesting that the model has consolidated spatial information into node representations, making explicit directional history redundant. The simple Layer 1 attention pattern appears to be a foundational building block used in two distinct ways: as a standalone heuristic for simple problems and as the initial input to a deeper, multi-layer computation for complex ones. 

This dual strategy suggests how transformers can develop hierarchical reasoning capabilities. Rather than learning a single, fixed algorithm, the Foraging Model appears to have developed an adaptive system that switches between computational strategies based on information availability. The model's ability to maintain high performance across diverse contexts suggests that it has learned not just spatial reasoning, but strategies for selecting appropriate reasoning approaches.

\section{Training Paradigms as Algorithmic Scaffolding}

Our comparative analysis shows that training frameworks act as algorithmic scaffolding, shaping not just performance but the computational strategies a model develops. On one end of the spectrum, we have the Foraging Model. Its three-stage processing pipeline (directional processing, spatial integration, then functional refinement) seems to have emerged from the inductive bias of passive, exploratory learning on high-variance random walks. Without explicit goals or structured data, the model appears to have developed a reusable representation of spatial structure. The emergence of a self-sufficient coordinate system by Layer 7 suggests how exploratory objectives may foster the development of allocentric, map-like representations that support flexible generalisation. 

In contrast, the Shortest Path models appear to have developed continuous, path-dependent computational strategies that never achieve the self-sufficiency observed in the Foraging Model. The direction ablation experiments revealed that both SP-Hamiltonian and SP-Random Walk models remain continuously dependent on explicit directional information throughout all 12 layers, with gradual recovery rather than the sharp phase transition observed in the Foraging Model, perhaps implementing something similar to procedural path integration. The SP-Hamiltonian model illustrates how goal-directed training on highly structured data can lead to specialised but brittle computational strategies. The model's horizontal mirroring in its representations is indicative of a strategy that exploits the regularities in its training distribution (the symmetric, structured nature of Hamiltonian paths) which breaks down when confronted with unstructured data or novel spatial configurations. 

The SP-Random Walk model's intermediate position on this spectrum demonstrates the nuanced effects of fine-tuning. By training on random walks, it was forced to abandon SP-H's brittle shortcuts, leading to better generalisation to larger grids. This suggests that fine-tuning on
random walks encourages the formation of reusable map-like representations. However, despite developing representations more similar to the Foraging Model, mechanistic analysis suggests that it retains the same continuous, path-dependent computational strategy as SP-Hamiltonian. The robustness improvement appears to stem from representational adaptation rather than algorithmic change, where the existing algorithm learned to work with more flexible representations while maintaining the same mechanistic dependencies.  However, the comparison between models is complicated by task difficulty differences. The SP models face inherently harder tasks (finding optimal paths) compared to the Foraging Model's simpler next-step prediction. While the Foraging Model shows superior generalisation to larger grids and novel spatial configurations, this may reflect both its training paradigm and the relative simplicity of its core task. The SP models' path-dependent strategies, while less generalisable, may be more appropriate for their specific goal-directed objectives. 

% However, the 29th node transition, where representations collapse into entangled clusters beyond position 29, is perhaps revealing of the limitations of this hybrid approach. The model's representational strategy becomes less cleanly defined, suggesting that competing pressures from different training paradigms may prevent convergence to a stable spatial encoding. Although, we must note the limitations of this interpretation. The cause of this is unclear, but as it does not correspond to a drop in task performance, it may be an artefact of the variable-length training data rather than a core part of the model's spatial algorithm. 



\section{Broader Implications for AI Development}

The findings of this investigation have implications for how we design and train AI systems for complex reasoning tasks. The exploration-exploitation trade-off observed in biological systems appears to apply to artificial neural networks as well. Training exclusively on optimal, expert demonstrations may lead to powerful but narrow strategies that fail to generalise to novel situations. The SP-Hamiltonian model's brittleness despite perfect in-distribution performance illustrates this principle clearly. Conversely, incorporating exploration, suboptimal data, and randomness into training paradigms may be essential for encouraging the development of robust, generalisable world models. The Foraging Model's success in developing cognitive map-like representations from random walks suggests that the apparent `inefficiency' of passive exploration may be a necessary catalyst for structural learning. 

The discovery of adaptive computational strategies in the Foraging Model suggests that robust AI systems may need to develop meta-cognitive capabilities—the ability to select appropriate reasoning strategies based on available information. This adaptive approach, rather than fixed algorithms, may be key to building systems that can handle diverse and novel situations. The fine-tuning analysis reveals important insights about how to improve existing systems. Rather than expecting fine-tuning to fundamentally alter learned algorithms, we should anticipate representational adaptation that allows existing computational strategies to work with new data distributions. 

\section{Limitations and Future Directions}

Several important limitations constrain the scope of these findings and suggest directions for future research. The analysis is limited to GPT-2 Small architecture and 4×4 grid environments, raising questions about how these patterns would extend to larger models or more complex environments. The mechanistic analysis of all models, but particularly the Shortest Path models, remains incomplete. While we have suggested that these models rely on continuous, path-dependent computation, we have not confirmed nor fully reverse-engineered the specific computations that perform path planning. Future work could use more granular patching experiments—for instance, patching the representations of start and goal tokens—to trace how models select and generate optimal paths. The 29th node transition in the SP-RW model represents an intriguing phenomenon that remains unexplained. Whether this reflects the limited strength of fine-tuning, competing pressures from different training paradigms, or some other mechanism requires further investigation. 

The generalisability of these findings to other domains and architectures remains to be established. While spatial reasoning provides an ideal testbed for mechanistic analysis, it remains unclear how these insights would apply to other cognitive capabilities or different model architectures. Future research could extend this analysis to larger models, more complex environments, and different cognitive domains. The methodological framework developed here—combining behavioural, representational, and mechanistic analysis—could be applied to study other emergent capabilities in neural networks. The connection to biological intelligence remains largely unexplored. While the findings suggest parallels between artificial and biological learning, deeper investigation of these connections could provide insights into both artificial and biological intelligence.

