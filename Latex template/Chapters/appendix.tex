\chapter{Appendices}


% \subsection{Context Length Robustness Analysis}

% This section provides detailed analysis of how model performance varies with context length, complementing the main results presented in Chapters 4 and 5.

% \subsubsection{Foraging Model Context Length Performance}

% The Foraging Model was trained on 120-step random walks and shows robust performance up to its training context length. However, performance degrades significantly when tested beyond this limit, demonstrating the classic transformer limitation of context length extrapolation.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/foraging_context_length.png}
% \caption{Foraging Model performance across different context lengths. The model maintains high accuracy up to its training context length of 120 steps, then shows rapid degradation beyond this limit.}
% \label{fig:foraging_context_length}
% \end{figure}

% \subsubsection{SP Models Context Length Analysis}

% Both SP-Hamiltonian and SP-Random Walk models show different patterns of context length sensitivity, reflecting their different training paradigms.
\section{Loss Curves and Training Dynamics}

Analysis of training loss curves reveals interesting phase transitions during training, particularly for the SP-Hamiltonian model. Detailed examination of the model's output during training reveals a clear progression through four distinct learning phases that correspond to the loss curve transitions.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/splosscombined.png}
\caption[Training loss curves for SP models]{\textbf{Training loss curves for SP models showing convergence patterns and phase transitions.} SP-Hamiltonian shows multiple phase changes during training, while SP-RW (fine-tuned from SPH) shows smoother convergence.}
\label{fig:training_loss_curves}
\end{figure}

Observations of the SP-Hamiltonian model's output during training suggest a phases of learning progression that appears to correspond to the loss curve transitions. In the first phase, the model appears to learn the basic node-direction-node syntax pattern, producing outputs that follow the correct grammatical structure but with random content. The second phase shows the model beginning to correctly predict start and end nodes while the middle path remains nonsensical, potentially indicating initial spatial boundary learning. The third phase represents what appears to be a critical transition where the model begins to get directions correct but hallucinates invalid node names, possibly suggesting attention mechanism failures during intermediate path generation. Finally, the fourth phase demonstrates the emergence of valid complete paths as the model appears to successfully integrate spatial reasoning with proper node prediction. This observed progression does not necessarily reflect the model's final solution strategy but rather may reveal how it converges to spatial reasoning capabilities through what appears to be incremental learning of different task components.

In contrast, the SP-Random Walk model exhibits no such phase transitions during training, showing smooth convergence without the phase transitions observed in SP-Hamiltonian. This difference supports the claim in Chapter 4 that the this model does not undergo algorithmic changes during learning, instead learning to adapt its strategy for random walk contexts. 

\section{Loop Completion Task Templates}

The loop completion tasks used throughout this thesis follow systematic templates that create geometrically valid paths returning to their starting nodes. These templates are designed to test the models' ability to understand spatial relationships and geometric constraints across different levels of complexity.

\begin{table}[H]
\centering
\caption[Loop completion task templates]{Loop completion task templates for different hop counts. Each template uses placeholders (\{\}) for node names and creates geometrically valid paths that return to the starting node. The final placeholder represents the target node that completes the loop. Direction abbreviations: E=EAST, W=WEST, N=NORTH, S=SOUTH.}
\label{tab:loop_templates}
\small
\begin{tabular}{cl}
\hline
\textbf{Hops} & \textbf{Template Examples} \\
\hline
2 & \texttt{\{\} E \{\} W \{\}} \\
  & \texttt{\{\} N \{\} S \{\}} \\
\hline
4 & \texttt{\{\} E \{\} S \{\} W \{\} N \{\}} \\
  & \texttt{\{\} N \{\} E \{\} S \{\} W \{\}} \\
\hline
6 & \texttt{\{\} E \{\} E \{\} N \{\} W \{\} W \{\} S \{\}} \\
  & \texttt{\{\} N \{\} N \{\} W \{\} S \{\} S \{\} E \{\}} \\
\hline
8 & \texttt{\{\} E \{\} E \{\} E \{\} S \{\} W \{\} W \{\} W \{\} N \{\}} \\
  & \texttt{\{\} S \{\} S \{\} S \{\} E \{\} N \{\} N \{\} N \{\} W \{\}} \\
\hline
10 & \texttt{\{\} E \{\} E \{\} E \{\} S \{\} S \{\} W \{\} W \{\} W \{\} N \{\} N \{\}} \\
  & \texttt{\{\} S \{\} S \{\} S \{\} E \{\} E \{\} N \{\} N \{\} N \{\} W \{\} W \{\}} \\
\hline
12 & \texttt{\{\} E \{\} E \{\} E \{\} S \{\} S \{\} S \{\} W \{\} W \{\} W \{\} N \{\} N \{\} N \{\}} \\
  & \texttt{\{\} S \{\} S \{\} S \{\} E \{\} E \{\} E \{\} N \{\} N \{\} N \{\} W \{\} W \{\} W \{\}} \\
\hline
\end{tabular}
\end{table}

These templates create systematic variations that test different aspects of spatial reasoning. The 2-hop loops represent simple reversal patterns that can be solved using local heuristics, while longer loops (4-12 hops) require more sophisticated spatial understanding and global reasoning. Each template is designed to create geometrically valid paths on a grid, ensuring that the models must understand spatial relationships rather than simply memorizing token sequences. The systematic nature of these templates allows for controlled testing of how model performance scales with task complexity, providing insights into the computational strategies employed by different models.


\section{Additional Experimental Results}
\subsection{Extended PCA Analysis}

% \subsubsection{Task Node Representations}

% While the main analysis focused on context node representations, this section examines how task nodes (start and goal nodes) are represented in the different models.

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{figures/task_node_pca.png}
% \caption{PCA analysis of task node representations across all models. Task nodes show different clustering patterns compared to context nodes, reflecting their special role in goal-directed planning.}
% \label{fig:task_node_pca}
% \end{figure}

% \subsubsection{Layer-wise Evolution of Representations}

Detailed PCA analysis examining representational evolution across all 12 transformer layers for both the Foraging Model and SP-Random Walk model. The analysis uses un-averaged node token representations extracted from 1000 unique random walks of length 50 on 3×3 grids.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{figures/combined.png}
\caption[Detailed PCA Analysis of the SP-RW Model.]{\textbf{Detailed PCA Analysis of SP-RW Model.} Three columns show layers 1, 7, and 12 (left to right). For each layer, the top row shows points coloured by arrival direction (AD), the middle row shows points coloured by coordinates, and the bottom row shows points coloured by path index.}
\label{fig:sprw_detailed_pca}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/combinedforaging.png}
\caption[Detailed PCA Analysis of the Foraging Model]{\textbf{Detailed PCA Analysis of the Foraging Model.} Three columns show layers 1, 7, and 12 (left to right). For each layer, the top row shows points coloured by arrival direction (AD), the middle row shows points coloured by coordinates, and the bottom row shows points coloured by path index.}
\label{fig:foraging_detailed_pca}
\end{figure}

Both models exhibit similar initial representational patterns in Layer 1, with four distinct clusters corresponding to each arrival direction and start nodes forming separate clusters. However, the models diverge in their representational evolution across layers. The Foraging Model undergoes a computational transition between layers 7 and 12, where the organising principle shifts from coordinate-based spatial structure to functional clustering based on available navigational directions. This transition reflects the model's progression from spatial position encoding to action-oriented representation. In contrast, the SP-Random Walk model maintains remarkably stable representational patterns across all layers, showing only minor geometric transformations (slight cluster shearing) without fundamental changes in organisational structure. This stability suggests that SP-RW develops a consistent representational strategy that remains effective throughout the network.


% \section{Training Details and Hyperparameters}

% \subsection{Model Architecture Specifications}

% All models use the GPT-2 Small architecture with the following specifications:
% \begin{itemize}
% \item Parameters: 124M
% \item Layers: 12
% \item Attention heads: 12
% \item Hidden dimension: 768
% \item Vocabulary size: Custom BPE tokenizer
% \end{itemize}

% \subsection{Training Configuration Details}

% \begin{table}[h]
% \centering
% \caption{Detailed Training Configuration}
% \label{tab:detailed_training_config}
% \begin{tabular}{lccc}
% \hline
% Parameter & Foraging & SP-Hamiltonian & SP-RW \\
% \hline
% Batch Size & 64 & 256 & 128 \\
% Learning Rate & 5e-4 & 5e-4 & 5e-4 \\
% Epochs & 2 & 12 & 12+20 \\
% Optimizer & AdamW & AdamW & AdamW \\
% Weight Decay & 0.01 & 0.01 & 0.01 \\
% Warmup Steps & 1000 & 1000 & 1000 \\
% Context Length & 120 & 16 & 10-50 \\
% Training Examples & 1M & 1M & 1M \\
% Gradient Clipping & 1.0 & 1.0 & 1.0 \\
% Dropout & 0.1 & 0.1 & 0.1 \\
% \hline
% \end{tabular}
% \end{table}

\subsection{Layer Redundancy and Ablation Analysis}

This section presents comprehensive layer ablation analyses to understand the redundancy and criticality of different layers in the Foraging Model. We perform two complementary analyses: (1) individual layer ablation to understand the contribution of each layer to overall performance, and (2) systematic ablation of all possible layer combinations to identify minimal required layers.

\subsubsection{Individual Layer Ablation}

We tested all 12 layers individually using 500 evaluation cases, measuring next-token prediction accuracy for each ablation. We systematically zeroed out layer outputs during inference, with 95\% confidence intervals calculated for binomial proportions.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/individuallayerabl.png}
\caption[Individual layer ablation results]{\textbf{Individual layer ablation analysis showing the impact of ablating each layer on task accuracy.} Layer 1 is absolutely critical (0\% accuracy when ablated), while layers 5-12 are individually redundant (100\% accuracy maintained). Layers 2-4 show minor contributions with slight performance drops.}
\label{fig:individual_layer_ablation}
\end{figure}

The results reveal that Layer 1 is indispensable, causing complete performance collapse (0\% accuracy) when ablated, highlighting its foundational role in processing information for the task. Layers 5-12 seem to be redundant, with zero impact on performance (100\% accuracy maintained), indicating that 67\% of the model's layers can be individually ablated with no impact on task perfofrmance. Layers 2-4 show minor contributions with small performance drops (96-99\% accuracy). 

\subsection{Systematic Layer Combination Ablation}

We tested all possible combinations of $k$ layers ($k = 1$ to 11) using 50 evaluation cases, totalling $\sum_{k=1}^{11} C(11,k) = 2^{11} - 1 = 2047$ combinations. Crucially, we exclude Layer 1 from this experiment, since it was found to be indispensable. This systematic approach reveals how the model's performance degrades as more layers are removed simultaneously.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/layerablation.png}
\caption[Systematic layer combination ablation results]{\textbf{Box plot showing the relationship between the number of ablated layers and model accuracy.} The plot demonstrates a strong inverse relationship: as more layers are ablated, performance degrades significantly. The model can tolerate ablating 1-2 layers with minimal impact, but ablating 5+ layers leads to severe performance collapse.}
\label{fig:systematic_layer_ablation}
\end{figure}

The systematic ablation reveals a clear performance degradation pattern: ablating 1-2 layers has minimal impact (95-100\% accuracy), 3-4 layers causes moderate degradation (75-85\% accuracy), 5+ layers leads to severe performance collapse (50\% or below), and 9+ layers results in near-complete failure (5\% or below). While individual later layers are redundant, the model still requires multiple layers working together, supporting the hypothesis of distributed rather than localised computation.


The analysis reveals significant overparameterisation, with 67\% of layers being completely redundant for individual ablation. Only Layer 1 is crucial for reasoning performance, suggesting the model could potentially be compressed without significant performance loss. Individual ablation results support the Chapter 4 finding that Layer 1 implements critical directional processing circuits, while the redundancy of layers 5-12 supports the claim that Layer 7 represents a transition point where the coordinate system becomes self-sufficient. Layer redundancy suggests distributed rather than localised computation, consistent with the three-stage processing pipeline identified in Chapter 4. Limitations include task specificity, the ablation method potentially not reflecting natural layer importance, and interaction effects between layers.


\subsection{Investigation of the 29th Node Effect}

The SP-Random Walk model exhibits a puzzling representational transition at the 29th node position in context walks, where node representations collapse from clean arrival-direction clustering into entangled clusters with no obvious organisational principle. To investigate whether this representational change corresponds to functional limitations, we designed three targeted memory tests that probe the model's ability to use information from different temporal positions in the context walk.

We tested three conditions: (1) \textit{Both Nodes Late}: Both start and goal nodes appear after position 29 in the context walk, (2) \textit{Start Early Goal Late}: Start node appears before position 29, goal node after, and (3) \textit{Constrained Expansion}: Context walk is constrained to a 3×3 inner grid for the first 29 positions, then expands to the full 4×4 grid. Each condition was tested on 1000 evaluation cases using the SP-RW model.

\begin{table}[H]
\centering
\caption[Analysis results for SP-Random Walk model]{\textbf{Performance on tests probing the 29th node transition effect.} All conditions show high accuracy (92-98\%), indicating that the representational collapse at position 29 does not impair functional performance.}
\label{tab:memory_analysis_results}
\begin{tabular}{lcc}
\hline
\textbf{Test Condition} & \textbf{Accuracy}\\
\hline
Both Nodes Late & 93.20\% \\
Start Early Goal Late & 92.80\% \\
Constrained Expansion & 97.60\% \\
\hline
\end{tabular}
\end{table}

The results demonstrate that the 29th node representational transition does not correspond to functional limitations. All three memory tests show high accuracy (92-98\%), with the constrained expansion test achieving the highest performance (97.60\%). This suggests that the representational collapse observed in PCA analysis represents a change in how information is encoded rather than a loss of functional capability. The model maintains robust spatial reasoning performance regardless of whether critical nodes appear before or after the 29th position threshold, indicating that the transition is likely a representational artifact rather than a fundamental computational limitation. This finding supports the interpretation that the 29th node effect reflects the model's adaptation to variable-length training data rather than a breakdown in spatial reasoning capabilities.

\subsection{Extended Direction Ablation Analysis}

To further investigate the SP-Hamiltonian model's directional processing, we conducted extended ablation experiments that selectively ablate different subsets of directional tokens. This analysis tests whether the model treats all four cardinal directions equivalently or shows asymmetric processing patterns.

We tested three ablation conditions: (1) ablating all directional tokens (NORTH, SOUTH, EAST, WEST), (2) ablating only NORTH/SOUTH tokens, and (3) ablating only EAST/WEST tokens. For each condition, we systematically ablated directional tokens at each layer and measured shortest path accuracy on 500 test cases.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/stratifiedDASPH.png}
\caption[Extended direction ablation analysis for SP-Hamiltonian model]{\textbf{Direction ablation analysis showing the impact of ablating different subsets of directional tokens across layers.} The identical performance curves for NORTH/SOUTH and EAST/WEST ablation demonstrate that the model treats all four cardinal directions equivalently, supporting claims about symmetric directional processing.}
\label{fig:direction_ablation_extended}
\end{figure}

The results demonstrate that the SP-Hamiltonian model treats all four cardinal directions equivalently. Ablating NORTH/SOUTH tokens produces identical performance degradation to ablating EAST/WEST tokens across all layers, with both conditions showing the same gradual recovery pattern. This symmetric processing supports the claim that the model's functional use of directional information is globally symmetric, even though PCA analysis showed subtle representational differences along one axis. The finding contradicts any interpretation that the model has asymmetric directional processing capabilities, confirming that the horizontal mirroring observed in PCA represents a representational artifact rather than functional asymmetry.

% \section{Statistical Analysis and Significance Testing}

% \subsection{Performance Comparison Statistical Tests}

% Detailed statistical analysis of performance differences between models across different tasks and conditions.

% \subsection{Confidence Intervals and Error Bars}

% All performance metrics reported in the main text include 95\% confidence intervals based on multiple evaluation runs.

% \section{Code and Implementation Details}

% \subsection{Data Generation Pipeline}

% The spatial navigation data was generated using a custom pipeline that creates:
% \begin{itemize}
% \item Random walk sequences for the Foraging Model
% \item Hamiltonian paths for SP-Hamiltonian training
% \item Variable-length random walks for SP-RW fine-tuning
% \end{itemize}

% \subsection{Evaluation Framework}

% The evaluation framework includes:
% \begin{itemize}
% \item Automated task generation
% \item Performance metric calculation
% \item Statistical significance testing
% \item Visualization tools for PCA analysis
% \end{itemize}

% \subsection{Mechanistic Analysis Tools}

% Custom tools developed for mechanistic interpretability analysis:
% \begin{itemize}
% \item Activation patching framework
% \item Ablation study automation
% \item Attention pattern visualization
% \item Representation analysis pipeline
% \end{itemize}

% \section{Additional Figures and Visualizations}

% \subsection{Extended Attention Analysis}

% Detailed attention pattern analysis for all models across different layers and tasks.

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{figures/attention_patterns.png}
% \caption{Attention pattern analysis showing how different models attend to spatial information across layers.}
% \label{fig:attention_patterns}
% \end{figure}

% \subsection{Representation Similarity Analysis}

% Analysis of how similar representations are across different models and conditions.

% \section{Limitations and Future Work}

% \subsection{Current Limitations}

% \begin{itemize}
% \item Analysis limited to 4×4 and 5×5 grids
% \item Single architecture (GPT-2 Small) examined
% \item Limited to spatial navigation domain
% \item Mechanistic analysis not fully complete for SP models
% \end{itemize}

% \subsection{Future Research Directions}

% \begin{itemize}
% \item Extension to larger grid sizes and more complex environments
% \item Analysis of different transformer architectures
% \item Investigation of other spatial reasoning tasks
% \item Complete mechanistic reverse-engineering of SP models
% \item Analysis of the 29th node transition mechanism
% \end{itemize}

% \section{Reproducibility Information}

% \subsection{Computing Environment}

% \begin{itemize}
% \item Hardware: [Specify GPU/CPU details]
% \item Software: Python 3.8+, PyTorch 1.12+, Transformers 4.20+
% \item Operating System: [Specify OS]
% \end{itemize}

% \subsection{Data and Code Availability}

% All code, data, and trained models will be made available upon publication to ensure full reproducibility of the results.

% \subsection{Hyperparameter Sensitivity}

% Analysis of how sensitive the results are to different hyperparameter choices, particularly learning rate, batch size, and model architecture variations.
