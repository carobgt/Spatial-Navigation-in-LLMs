\chapter{Background \& Related Work}

The ability of large language models to perform complex reasoning tasks, despite being trained on the simple objective of next-token prediction, is a key area of investigation in modern artificial intelligence. This capability raises a question regarding the algorithms learned by these networks: whether they acquire flexible, generalisable internal models of the world, or instead rely on a large set of task-specific, brittle heuristics. This question can be framed by examining its origins in cognitive science and its contemporary investigation through mechanistic interpretability, providing a framework for understanding how different training approaches influence the learning process in transformers. This section reviews the concepts of cognitive maps and emergent world models, contrasting the inductive biases of passive, exploratory learning with the optimisation pressures of active, goal-directed training.

\section{Planning in Large Language Models}

This section focuses on the application of transformers to spatial navigation and planning tasks. The use of simplified, synthetic environments has become a common method for understanding the internal mechanisms of these models. This approach allows for controlled experimentation and detailed analysis of the algorithms that transformers learn. However, this line of inquiry has also revealed limitations of the standard autoregressive paradigm, particularly in tasks that require multi-step reasoning.

\subsection{Synthetic Worlds as Mechanistic Testbeds for Spatial Reasoning}

The use of synthetic grid worlds, mazes, and other algorithmic tasks as experimental environments is a well-established and highly effective framework in AI research for probing the reasoning capabilities of neural networks \cite{trigonometry, bachmann2024ntp, chughtai2023toymodel, nanda2023progress, nolte2024spatial}. These environments offer an advantage over complex, real-world data in that they are fully specified and controllable. This allows researchers to systematically vary task parameters—such as grid size, path complexity, or the amount of available information—and observe the corresponding effects on model behaviour and internal representations. This level of control is essential for performing the kind of rigorous, mechanistic analysis that aims to reverse-engineer a model's learned algorithm. For instance, Nolte et al. (2024) trained transformers from scratch to navigate mazes of varying complexity, isolating the effect of the training objective on planning capabilities \citep{nolte2024spatial}. 

\subsection{The Fragility of Planning in Autoregressive Models}

While transformers can be trained to perform planning tasks, the standard next-token prediction (NTP) objective, particularly when combined with teacher-forcing during training, has inherent limitations that can lead to non-generalisable solutions \cite{bachmann2024ntp}.

One of the most significant challenges is the problem of compositional generalisation \citep{montaguecompositionality}. This refers to the ability to combine known components into novel structures to solve new problems. Research has shown that while transformers may learn to execute individual, single-step operations seen during training, they often fail to compose these operations into correct, multi-step algorithms when faced with novel or more complex problem instances \cite{lake2018generalizationsystematicitycompositionalskills, zhou2023leasttomostpromptingenablescomplex, dziri2023faithfatelimitstransformers}.

A theoretical explanation for this failure is that teacher-forcing induces shortcuts. Teacher-forcing is the standard training procedure for autoregressive models, where the model is trained to predict the next token, $y_t$, given the ground-truth prefix, $y_{1:t-1}$. This creates a discrepancy between the training and inference conditions. During inference, the model must generate tokens autoregressively, conditioning its predictions on its own, potentially erroneous, previous outputs. This mismatch, known as `exposure bias,' can lead to a `snowballing' or compounding of errors, where a single mistake can derail the entire generation process, leading to catastrophic failures in long-horizon tasks. While this critique of NTP is well-established, a deeper limitation lies within the training process itself. 

Recent work has argued that the teacher-forcing objective actively disincentivises the learning of robust, generalisable algorithms for tasks that require planning or lookahead. As articulated by Bachmann \& Nagarajan (2024), NTP encourages the model to learn simple, local heuristics rather than engaging in multi-step planning \cite{bachmann2024ntp}. In lookahead tasks, where a later token in a sequence must be implicitly planned before an earlier token can be correctly generated, teacher-forcing provides a shortcut. By revealing the ground-truth prefix, the model learns to answer the question, `What token is statistically likely to follow this prefix of the correct answer?' instead of, `Given the problem statement, what is the first step of the correct solution?' This creates a `Clever Hans cheat,' where the model appears competent by exploiting superficial cues in the teacher-forced context, but has not learned the underlying algorithm required for robust generalisation. 

The reliance on local statistics and shortcuts has been characterised by Dziri et al. (2023) as a process of reducing complex, multi-step compositional reasoning into `linearised subgraph matching' \citep{dziri2023faithfatelimitstransformers}. Instead of learning a systematic, rule-based problem-solving procedure, the model learns to match fragments of the current problem to similar sub-problems and their corresponding solution fragments seen during training. This approach can achieve high performance on in-distribution data where such subgraphs are frequent, but it fails to generalise to more complex or novel instances that require a different compositional structure. Dziri et al. (2023) provide a comprehensive analysis of this failure across several compositional tasks, including multi-digit multiplication and logic puzzles. By representing tasks as computation graphs, they quantify complexity via `reasoning depth' (the number of sequential steps) and `width' (the number of parallel items to maintain). They find that performance, even for state-of-the-art models like GPT-4, degrades exponentially with reasoning depth. For instance, while models excel at 2x2 digit multiplication, their accuracy plummets on 3x3 digit problems \citep{dziri2023faithfatelimitstransformers}.

In spatial navigation specifically, Nolte et al. (2024) note that transformers trained with a standard NTP objective on maze-solving tasks struggle significantly as maze size and path complexity increase \citep{nolte2024spatial}. The models often fall prey to the `Clever Hans' shortcuts predicted by theory, learning to follow local cues rather than developing a global plan. Their performance saturates at low accuracy levels for complex mazes, indicating a failure to generalise the navigation skill.

\subsection{Alternatives to Next-Token Prediction}

In response to the well-documented limitations of standard next-token prediction, researchers have developed alternative training objectives designed to foster more robust planning and reasoning capabilities.

One promising direction is the use of multi-token prediction objectives. Instead of predicting only the single next token, these methods train the model to predict multiple future tokens simultaneously \cite{gloeckle2024betterfasterlarge}. This forces the model to look further ahead, creating a stronger learning signal for long-term dependencies. For example, the MLM-U objective, which involves masking and predicting arbitrary subsets of a sequence, has been shown to improve the ability of transformers to navigate complex mazes compared to standard next-token prediction \citep{kitouni2024factorizationcursetokenspredict}.

This leads back to a more nuanced understanding of goal-conditioned models like the Decision Transformer \cite{DT}. These models can be viewed not just as a way to apply transformers to RL, but as a solution to the inherent planning failures of standard autoregressive models. A standard next-token predictor is given only a history and must infer the goal implicitly. This is a difficult, under-specified problem. A goal-conditioned model, in contrast, is explicitly provided with the global context it needs to plan effectively.

The choice of training objective can be seen as a form of algorithmic scaffolding, constraining and guiding the search space of possible algorithms that the model can learn. A standard, local, next-token prediction objective provides a backward-looking signal, scaffolding the learning of reactive, path-dependent algorithms that are prone to shortcuts. In contrast, a goal-conditioned objective provides a global, forward-looking signal, scaffolding the learning of planning algorithms that can causally link a current state to a desired future state \cite{nasiriany2019planninggoalconditionedpolicies}. Meanwhile, an exploratory objective on high-entropy, unbiased data provides a dense signal covering the entire state space. This scaffolds the learning of a complete world model, as building a single, unified representation is the most efficient way to handle diverse and unstructured data.

\section{Cognitive Maps and Emergent World Representations}

The distinction between learning a general environmental model versus specific routes was first investigated in the mid-20th century by psychologist Edward Tolman. Through a series of experiments, Tolman challenged the prevailing behaviourist view that learning was merely the formation of stimulus-response associations. He proposed that animals, specifically rats navigating mazes, construct an internal `cognitive map'—a comprehensive, map-like representation of their environment, rather than learning specific responses to individual stimuli \citep{tolman1948cognitive}. In his experiments, rats that were allowed to passively explore a maze without any reward later demonstrated the ability to find efficient, novel shortcuts to a food source when their familiar path was blocked. This behaviour could not be explained by a simple chain of learned motor responses; instead, it suggested the rats had developed a flexible, allocentric (i.e., world-centred and viewpoint-independent) model of the maze's spatial layout \citep{tolman1930maze}.

A key concept arising from this work was \textit{latent learning}: the acquisition of knowledge that is not immediately apparent in an animal's behaviour but manifests when a suitable motivation or task is introduced. The rats learned the structure of the maze during the exploratory phase, even without reinforcement, and later applied this knowledge in a goal-directed context \citep{okeefe1978hippocampus, BEHRENS2018490, whittington2022howtobuild}.

This classic concept from cognitive science has found a modern analogue in the study of emergent world representations in large language models. A growing body of research suggests that transformers, even when trained on simple sequence prediction tasks, can develop internal models of the underlying data-generating process. Spens \& Burgess (2024) propose a framework where this exact process is simulated by training a GPT-style transformer on various types of sequential data, including navigation path sequences \citep{Spens2024consolidation}. They propose a model of memory consolidation, inspired by the brain, where a large generative network (analogous to the neocortex) is trained by repeatedly replaying sequential experiences that were first stored in a rapid-learning memory buffer (the hippocampus). The core insight is that the simple, self-supervised objective of predicting the next item in a sequence forces the network to extract the underlying statistical regularities and structure from these experiences. Over time, this process distills specific, episodic traces into a generalised schema or `cognitive map' of the environment's structure. As noted in the paper, this is interesting because decoder-only models, which lack an explicit architecture for latent variables (such as variational autoencoders, \citep{kingma2022autoencodingvariationalbayes}), still manage to learn a sophisticated world model implicitly \citep{Spens2024consolidation}. 

Another notable study in this area is the work on Othello-GPT, a model trained exclusively to predict legal moves in the game of Othello \citep{li2024emergentworldrepresentationsexploring}. Despite having no explicit knowledge of the game's rules or board structure, the model was found to develop an internal representation of the full 8×8 board state. Subsequent investigations revealed that this representation was not only present but also linear and causally linked to the model's output; by directly intervening on the model's hidden states to `flip' the representation of a piece on a specific square, researchers could reliably alter the model's subsequent move predictions to be consistent with the new, counterfactual board state \citep{nanda_othello_2023}. The Othello-GPT experiments provide evidence that a local, predictive objective can lead a model to learn a global, coherent world model as an efficient solution for minimising its prediction error.

Research in reinforcement learning has further explored the idea of `world models', where an agent explicitly learns a generative model of its environment in an unsupervised manner \citep{MATSUO2022267,ha}. As proposed by Ha and Schmidhuber, this learned world model—a compressed spatial and temporal representation—can then be used to train a much smaller, more efficient policy, sometimes entirely within the `dream' generated by the world model itself \citep{ha}. Here, a `dream' refers to simulated trajectories produced by the world model, allowing the agent to practice and improve its policy without interacting with the real environment. This explicitly separates the process of world-building (unsupervised, exploratory) from policy-learning (goal-directed, exploitative).

These distinct lines of research converge on a general principle. A passive, next-token prediction objective, when applied to a sufficiently rich and diverse dataset like random walks or game transcripts, appears to create a strong inductive bias towards the formation of a comprehensive world model. The model is not being optimised for any single, narrow goal but for general predictive accuracy across a vast state space. In such a regime, a parsimonious and computationally efficient strategy is not to memorise a vast collection of specific input-output patterns or heuristics, but to learn a single, unified model of the environment's underlying rules and structure. This learned model of the data-generating process is more generalisable and ultimately requires fewer parameters than a lookup table of surface-level statistics.

\subsection{Exploration vs. Exploitation}

The distinction between exploratory and goal-directed learning can be formalised within the exploration-exploitation framework from reinforcement learning \citep{explorationexploitation}. This framework describes the trade-off an agent faces between exploration—gathering new information about its environment to potentially discover better strategies—and exploitation—using its current knowledge to choose actions that are already known to yield high rewards. An agent that only exploits may get stuck in a suboptimal policy, while an agent that only explores may never capitalise on its discoveries.

This framing of reinforcement learning tasks as sequence modelling problems has gained significant traction with the rise of the transformer architecture. A key development in this area is goal-conditioned reinforcement learning (GCRL), where a policy is learned to achieve specified goals \citep{liu2022GCLR}. An example is the Decision Transformer (DT), which casts RL as a conditional sequence modelling problem \citep{DT}. Instead of learning value functions or policy gradients, the Decision Transformer is an autoregressive model that takes a sequence of past states, actions, and a desired return-to-go (the target cumulative reward) as input, and is trained to predict the next action in the sequence. By conditioning on a high desired return, the model can be prompted to generate a sequence of actions that constitutes an optimal policy. This demonstrates that explicitly providing a goal as part of the model's input is a powerful technique for eliciting specific, optimised behaviours from a sequence model.

\section{Mechanistic Interpretability: Deconstructing Learned Algorithms}

To move beyond observing what a model can do and understand how it does it, the field of mechanistic interpretability (MI) offers a suite of powerful techniques. This research paradigm aims to reverse-engineer the computational processes within neural networks, decomposing their operations into human-understandable algorithms \cite{rai2025}. This section reviews the core concepts of MI, including the search for `circuits', the use of causal interventions to validate hypotheses, and the analysis of the geometric properties of learned representations.

\subsection{Reverse-Engineering Transformer Circuits}

The central goal of mechanistic interpretability is to decompose a neural network's function into its constituent parts, often conceptualised as circuits. A circuit is a specific subgraph of the model's overall computational graph that is responsible for implementing a particular, human-interpretable function \cite{rai2025}.  This approach has been successful in identifying such circuits for a variety of tasks. One influential case studies is the discovery of the Indirect Object Identification (IOI) circuit in the GPT-2 Small model \cite{wang2022ioi}. In the IOI task, the model must complete a sentence like `When Mary and John went to the store, John gave a drink to...' with the correct indirect object, `Mary'. Researchers were able to identify a circuit of 26 specific attention heads that work in concert to solve this task. They categorised these heads into distinct functional groups, such as `Name Mover Heads' that copy the correct name to the final position, and `S-Inhibition Heads' that prevent the model from copying the subject's name. By tracing the flow of information between these components, they constructed a mechanistic explanation of a linguistic capability.

Another foundational discovery in MI is the identification of induction heads \cite{olsson2022context}. These are specialised attention heads that learn to recognise and complete repeating patterns. For example, given a sequence containing the pattern \texttt{A B}, an induction head will strongly attend to \texttt{B} when it later encounters \texttt{A}, effectively implementing the algorithm \dots \texttt{A B \dots A $\rightarrow$ B}.
\cite{olsson2022context}. The emergence of induction heads is hypothesised to be a key mechanism underlying the in-context learning capabilities of large language models \cite{chen2025suddendropsloss}.

\subsection{From Correlation to Causation: Probing and Intervening on Representations}

Mechanistic interpretability provides a methodology for moving from correlational observations about a model's internal states to causal claims about its computational algorithm. The first step typically involves correlational methods, which aim to characterise the structure of a model's internal representations. Techniques such as Principal Component Analysis (PCA) and linear probing are used to project the high-dimensional hidden states of the model into a lower-dimensional space where human-interpretable patterns may become visible \citep{alain2018understandingintermediatelayersusing}. These observational techniques are useful for generating hypotheses about what information the model is encoding.

To validate the functional role of these observed representations and the components that produce them, MI relies on causal methods. These techniques involve targeted interventions on the model's internal activations to test specific hypotheses about information flow. Activation patching, for instance, involves running the model on two different inputs (a `clean' input and a `corrupted' input) and swapping the activation of a specific component (e.g., an attention head's output) from the clean run into the corrupted run. If this patch restores the correct output on the corrupted input, it provides strong causal evidence that the patched component is necessary for the behaviour in question \cite{zhang2024bestpracticesactivationpatching}. A simpler form of intervention is ablation, where the output of a component is zeroed out or replaced with a mean value to observe its effect on performance. This progression from observing correlations to establishing causality exemplifies the approach of the MI framework.

\subsection{The Geometry of Learned Representations}

A line of inquiry within MI focuses on the specific geometric structures that emerge within the activation space of transformers. This research posits that models often learn to represent concepts not just as directions in a vector space, but as points on more complex, non-linear manifolds that are mathematically suited to the task at hand.

An example of this principle comes from work on transformers trained to perform modular arithmetic. Nanda et al. (2023) and subsequent work by Kantamneni \& Tegmark (2025) showed that these models learn to represent numbers on a generalised helix, a geometric structure composed of multiple circles of different frequencies corresponding to different moduli \cite{nanda2023progress, trigonometry, chughtai2023toymodel}. The model implements addition by performing rotations on this helix, using trigonometric identities to combine the representations of the input numbers \cite{trigonometry}c. This discovery is significant because it shows that a transformer can learn a non-trivial geometric representation that captures the underlying mathematical structure of the task.

\section{Synthesis and Positioning of the Current Research}

The preceding review highlights a tension in our understanding of transformer capabilities. On one hand, studies on tasks like maze-solving and multi-digit multiplication demonstrate that the standard autoregressive objective often leads models to acquire brittle, non-compositional heuristics. The `Clever Hans' shortcuts induced by teacher-forcing show that models can achieve high in-distribution performance by exploiting local statistical cues rather than learning a robust, underlying algorithm. On the other hand, research on emergent world models, such as the internal board state discovered in Othello-GPT, provides a modern computational analogue to Tolman's theory of cognitive maps \citep{tolman1948cognitive, li2024emergentworldrepresentationsexploring}. It suggests that a predictive objective on diverse, exploratory data can encourage a model to learn a coherent, allocentric representation of its environment as the most efficient compression of the data.

While these two outcomes—brittle heuristics versus generalisable world models—are well-documented, they have largely been studied in isolation. The critical gap lies in a direct, comparative analysis of the internal mechanisms that produce them. We understand that different data distributions and objectives lead to different capabilities, but we lack a mechanistic account of how the learned algorithms themselves differ.

This thesis is positioned to bridge this gap by conducting a comparative mechanistic analysis. We operationalise the distinction identified in the literature through two controlled experimental conditions:

\begin{itemize}
\item The \textbf{Foraging Model}, trained on random walks, directly simulates the conditions of passive, exploratory `latent learning' hypothesised to produce cognitive maps. Its objective is purely predictive, lacking an explicit goal.
\item The \textbf{Shortest Path models}, trained on an optimal planning task, exemplify goal-directed, exploitative learning where the model is optimised to solve a narrow, well-defined problem.
\end{itemize}

By applying the causal intervention tools of mechanistic interpretability to both model types, this work moves beyond simply cataloguing behavioural differences. The central aim is not to produce a complete reverse-engineering of the learned algorithms, but rather to focus on the mechanistic question of how these different models compute their solutions. By applying causal interventions like activation patching and targeted ablation, we can trace the flow of spatial information and identify critical differences in the computational strategies that emerge from each training objective. In doing so, this research aims to provide a concrete, mechanistic explanation for how training paradigms shape learned algorithms in the domain of spatial reasoning.
