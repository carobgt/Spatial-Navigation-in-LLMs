\chapter{The Foraging Model}

How does a model trained only to predict the next token in random walks develop sophisticated spatial reasoning? This chapter investigates spatial navigation in its simplest form: passive information integration through exploratory movement. The Foraging Model represents our exploration-based approach to spatial learning, trained solely on random walks without explicit goals. We trace the model's journey from local pattern matching to global spatial understanding through behavioural, representational, and mechanistic analyses.

The Foraging Model captures exploratory learning by training on random walks through grid environments. Unlike goal-directed navigation, this paradigm requires no explicit planning objective--the model simply learns to predict the next valid step in a sequence of movements. This minimal training setup allows us to isolate the core mechanisms of spatial learning and establish a baseline for understanding how different training approaches shape spatial reasoning.

Our investigation addresses several key questions: Can a model trained solely on random walks develop robust spatial understanding? What internal representations emerge to support spatial reasoning? How does the model transition from local pattern matching to global spatial understanding? By answering these questions, we establish the foundation for comparing different approaches to spatial learning in subsequent chapters.

% \section{TODO} will do this in appendix
% TODO: talk about phase changes, grokking and in-context learning, link to nanda lesswrong post, probably in discussion

%  with training exhibiting distinct phase transitions rather than gradual improvement---initially low accuracy during basic syntax learning, followed by an extended plateau around 90\%, then a sharp jump to near-perfect performance.

\section{Behavioural Analysis}

We evaluate the Foraging Model's spatial reasoning capabilities using the evaluation framework described in Chapter 3, assessing core performance metrics, context length robustness, and generalisation capabilities. Here, the `context' is the historical sequence of nodes and directions provided as the input prompt from which the model must infer its current position.

\subsection{In-Distribution Tasks}

We begin by assessing the model's performance on its core training objective: next-step prediction on 4$\times$4 grids. The Foraging Model achieves 98.8\% accuracy when predicting a valid move (direction and corresponding node) with a context length of 110 nodes (see Figure \ref{fig:foraging_performance_profile}A). However, when predicting only the next node (given both the current position and direction to move in), the model achieves perfect accuracy (100\%) across all context lengths.

The model's capabilities extend to tasks requiring global spatial reasoning that cannot rely on sequence memorisation. On Hamiltonian cycle completion, the model receives a complete path visiting all 16 nodes exactly once and must predict which node completes the cycle back to the starting position, achieving perfect accuracy (100\%). On simpler loop completion tasks, performance remains perfect (100\%) for patterns of 2-12 hops. Both tasks require inferring spatial relationships that may never have appeared in training, as the model must understand geometric constraints rather than rely on memorised sequences.

In multistep generation, the model shows a gradual decline in accuracy. When continuing 110-step context paths, performance drops from 98.8\% for 1-step generation (direction-node pair) to 97.6\% for 2-step generation and 94.2\% for longer continuations (10 steps). This decline is unsurprising, and reflects the absence of self-correction mechanisms--errors compound across generation steps without opportunity for recovery.


\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/foragingperformance.png}
\caption[Foraging Model performance across context lengths and grid sizes.]{\textbf{Foraging Model performance across context lengths and grid sizes.} (A) Single-step prediction accuracy (purple) and reverse bias (red) as a function of context length (2--115 steps), measured on 500 random walks per context length. The U-shaped accuracy curve shows transition from heuristic (high reverse bias at short contexts) to map-based reasoning (low reverse bias at long contexts). (B) Generalisation performance on larger grids: random walk continuation (1-step, CL=115) and N$\times$N square loop completion, tested on 100 trials per grid size. Performance degrades gracefully from 98.3\% (4$\times$4) to 50\% (6$\times$6) for random walks, while loop completion maintains 100\% accuracy up to 5$\times$5 grids.}
\label{fig:foraging_performance_profile}
\end{figure}

\subsection{Context Length Robustness}

One of the most striking behavioural findings is the Foraging Model's non-monotonic relationship between performance and context length (Figure \ref{fig:foraging_performance_profile}A). This pattern suggests that the model adapts its computational strategy depending on the amount of information available: short, intermediate, and long contexts elicit qualitatively different behaviours. 

To understand this transition, we analyse the model's reverse bias (its tendency to reverse the last move) which provides insight into the underlying reasoning strategy. Formally, given a walk ending with direction $d$, let $d_{rev}$ be its reverse (e.g., SOUTH for NORTH) and $\mathcal{D}_{valid}$ be the set of valid moves from the current node. The reverse bias $B$ is defined as:

\begin{equation}
B = P(d_{rev}) - \frac{1}{|\mathcal{D}_{valid}| - 1} \sum_{d' \in \mathcal{D}_{valid} \setminus \{d_{rev}\}} P(d')
\end{equation}

where $P(d)$ is the model's predicted probability for direction $d$. We computed this bias across 500 random walks for each context length from 2 to 115 nodes.

The analysis reveals three regimes:  

\begin{enumerate}
    \item \textbf{Minimal context (2–3 steps):} The model achieves near-perfect accuracy (100\%), but exhibits a strong preference for reversing the last move (reverse bias = 0.38). Here, the model relies on local heuristics: given very limited context and no global information about the grid, the safest move is often simply to reverse the previous step.

    \item \textbf{Intermediate context (5–40 steps):} Accuracy decreases (70–85\%), while reverse bias drops (0.16 at 11 steps). This regime may represent a transitional phase where the influence of local heuristics is reduced, and the model is exposed to a greater diversity of valid moves. Because the training data consisted of random walks with equal probability for each valid move, the model experiences a pull toward more uniform predictions across directions. This dip likely reflects the fact that intermediate-length walks are long enough to exceed minimal local heuristics but still too short to uniquely identify their position on the grid, creating uncertainty in the model’s predictions.
 

    \item \textbf{Long context (40+ steps):} Performance recovers to near-perfect levels ($>97\%$), with reverse bias approaching zero. With longer context, the model can condition its next-step predictions on a larger portion of the trajectory, producing consistent, context-informed behaviour that aligns with the overall walk structure.  
\end{enumerate}

In summary, the reverse bias analysis provides insight into how the model adapts its reasoning with context length. Short contexts favour local heuristics, intermediate contexts reflect a combination of heuristic and task-driven uncertainty, and long contexts allow robust, context-dependent navigation. While the model shows robust spatial generalisation, it exhibits sharp temporal boundaries. Performance remains high until the context length approaches the training limit of 120 steps, where it drops to 14\% accuracy.

\subsection{Generalisation Performance}

Despite training exclusively on 4×4 grids, the model generalises effectively to larger environments, though its success depends critically on the nature of the task (Figure \ref{fig:foraging_performance_profile}B). 

The model is notably better at tasks that have a deterministic, geometrically constrained answer. Performance on Square Loop Completion remains nearly perfect on grids up to 5x5 (100\%) and only degrades slightly on 7x7 grids (80.5\%). This makes sense. Completing a square loop is a puzzle with a single, geometrically correct answer. The model's robust performance shows it has learned an abstract spatial reasoning that transfers well to larger, unseen environments.

In contrast, its performance is weaker on tasks with stochastic, open-ended answers. Continuing a random walk on a large, unfamiliar grid is hard because there are often multiple valid moves. This forces the model to predict a direction where uncertainty is high, leading to a faster drop in accuracy as grid size increases (down to 63.5\% on 7x7 grids). This confirms that the model struggles more with predicting the next direction than the next node. Crucially, we can be confident this performance drop is not simply due to the autoregressive challenge of predicting two tokens (direction and node) versus one (node completing the loop). As established in our in-distribution analysis, the model achieves perfect 100\% accuracy on node-only prediction across all context lengths. This demonstrates that once a direction is given, the model deterministically infers the resulting node. Therefore, the weaker performance on random walk continuation stems directly from the uncertainty of the directional decision itself, not from compounding errors during generation.

\subsection{Summary}

The behavioural analysis reveals that the Foraging Model achieves strong performance across multiple spatial reasoning tasks. The model demonstrates perfect accuracy on node-only prediction tasks, maintains high performance on its training objective, and shows robust capabilities on abstract spatial reasoning tasks such as Hamiltonian cycle completion and loop completion. The model exhibits a non-monotonic relationship between performance and context length, with graceful degradation when generalising to larger grid sizes. These findings raise questions about the internal representations that support such performance. To investigate this, we next turn to a representational analysis of the model's hidden states.

\section{Representational Analysis}

%\subsection{Methods}

We examine the model's internal spatial representations using PCA and linear probing, as detailed in Chapter 3. For PCA analysis, we sample from 1,000 random walks on unique 3$\times$3 grids, averaging node representations across occurrences to control for positional effects. For linear probing, we train probes on 500 examples per layer to predict true (x,y) coordinates from hidden state representations of node tokens.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figures/pcaforagingl1l7.png}
\caption[PCA of node token hidden states in Layers 1 and 7.]{\textbf{PCA of node token hidden states in Layers 1 and 7.} Data from 1,000 random walks of length 120 on unique 3$\times$3 grids. Points coloured by grid coordinates (R,C) where R,C $\in$ \{0,1,2\}. Layer 1 (left) shows unstructured spatial representations, while Layer 7 (right) exhibits clear coordinate organisation with top two PCs aligning with grid axes (cosine similarity $\approx$ --0.0415), indicating the emergence of an orthogonal coordinate system.}
\label{fig:pca_layers_1_7}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figures/pcaforagingl12.png}
\caption[3D PCA of Layer 12 node token hidden states.]{\textbf{3D PCA of Layer 12 node token hidden states.} Data from 1,000 random walks of length 120 on unique 4$\times$4 grids. Nodes cluster by navigational affordances: corner nodes (2 available directions, N=4), edge nodes (3 available directions, N=8), and centre nodes (4 available directions, N=4). Functional clustering replaces coordinate organisation, with nodes clustering by possible moves rather than spatial position, demonstrating action-oriented representation.}
\label{fig:pca_layer_12}
\end{figure}

%\subsection{Results}

\subsection{Principal Component Analysis}

PCA reveals a three--stage evolution in the model's spatial representations (Figure \ref{fig:pca_layers_1_7}). Early layers (1--3) show a basic, noisy sense of coordinates, reflecting initial processing of local syntactic and positional information without coherent spatial structure. A transformation occurs around Layer 7, where node representations organise into clear spatial patterns. Consistent with Spens \& Burgess' findings, the top two principal components align closely with the grid's x and y axes (cosine similarity $\approx$ --0.0415), indicating development of an orthogonal, Cartesian-like coordinate system that represents spatial position independently of the specific path taken to reach it \citep{Spens2024consolidation}. At this stage, the model has a robust representation of where each node is. The clean coordinate system dissolves in late layers (11--12) and shifts to functional clustering based on navigational affordances (Figure \ref{fig:pca_layer_12}). The organizing principle shifts from spatial position to geometric function, with nodes clustering by their possible moves. Corner nodes, each having a unique pair of two available directions (e.g., south and east), form four distinct and well--separated clusters. Edge nodes also form four groups, with nodes positioned along the same edge clustering together. For example, all nodes on the top edge (from which one can move east, west, and south) form one group, separate from the nodes on the left edge (from which one can move north, south, and east). Center nodes, which are functionally identical as all four directions are available from them, converge into a single, tightly entangled group. This transformation suggests the model's final layers are not just tracking location but are computing a more abstract, action-oriented representation. This transition is consistent with the broader pattern in deep learning systems, where mid-layer representations preserve structured embeddings, and later layers reorganise them into task-relevant abstractions. For navigation, functional clustering by available moves is a more directly useful basis for prediction than spatial position.

\subsection{Linear Probing}

Linear probing confirms the PCA findings with quantitative precision. The R$^2$ score increases steadily through early layers, plateaus around Layer 5 (R$^2$ $\approx$ 0.9), and reaches its peak at Layer 8 (R$^2$ $\approx$ 0.93). This confirms that the model develops a robust, linearly--decodable coordinate system that stabilizes in the middle layers. The plateau at Layer 7 aligns precisely with the PCA results, providing converging evidence for when the spatial representation becomes established. However, a high R$^2$ score only proves that coordinate information is linearly present in the representation. It is a strong correlation, but it is not causal proof that the model makes use of this coordinate system. The continued high decodability through later layers indicates coordinate information persists even as the representation transforms toward functional clustering.


\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Latex template/figures/linearprobe.png}
\caption[Linear probing performance for coordinate decoding across transformer layers.]{\textbf{Linear probing performance for coordinate decoding across transformer layers.} R$^2$ scores computed on 500 examples per layer, training linear probes to predict (x,y) grid coordinates from hidden state representations. Performance increases from R$^2$=0.15 (Layer 1) to plateau at R$^2$$\approx$0.93 (Layer 8), suggesting coordinate system emergence. The plateau aligns with PCA findings, indicating stable spatial representation by middle layers.}
\label{fig:linear_probing}
\end{figure}


\subsection{Summary }

These observational findings point to a three--stage pipeline: (1) processing of local information, (2) integration into a coordinate map, and (3) refinement into an action-oriented map. However, these analyses are purely correlational; they show that spatial information is present and linearly decodable, but not that the model causally relies on it in this form. We now use causal interventions to test this hypothesis and uncover the underlying computational mechanism.

\section{Mechanistic Analysis}

Our representational analysis revealed the emergence of structured spatial representations, but correlation does not imply causation. To understand how the computational processes actually work, we now perform targeted interventions that manipulate information flow through the network. The goal of this mechanistic analysis is twofold: first, to test the specific hypotheses generated by our observational findings, and second, to uncover the underlying circuits that implement them. We will systematically dissect the model's computation to establish the causal role of different components in its spatial reasoning algorithm.

\subsection{Localising the Direction Update Circuit}

Our investigation began with searching for the simplest mechanism underlying spatial reasoning: the circuit that processes a single direction token (e.g.,`EAST') and updates the model’s internal spatial state. Understanding where these computations occur can reveal how the model processes directional information and updates its internal representation of space. We started with the simplest hypothesis: the model implements `go east' types of instructions as vector addition in MLPs.

\subsubsection{Minimal Pair Activation Patching}

To test our hypothesis, we constructed minimal pair prompts--identical random walk sequences except for their final direction token (e.g., one ending with `EAST' and another with `WEST'). The goal was to see which component, if any, could successfully compute the directional update. We then systematically patched activations from one prompt into the other to identify which components could successfully compute the directional update.

Using the activation patching methodology outlined in Chapter 3, we tested MLP outputs across all layers first. This failed completely, forcing us to reject our initial hypothesis, and suggesting that a more complex mechanism was at play. The failure of the MLPs is theoretically expected. By design, MLP layers process information at each token position independently. However, because node names are randomized in every sequence, this task is an instance of In-Context Learning (ICL). The model cannot memorize that `ab is west of cd'; it must infer this relationship from the context of the current prompt. The only mechanism capable of moving information between token positions to establish such context-dependent relationships is \textit{attention}. Therefore, we shifted our search to the attention blocks. Patching the attention output of Layer 1 successfully and consistently controlled the model's prediction, establishing this component as causally responsible for processing directional information.

Because both prompts only differed in their final direction, this finding raised a key ambiguity: was the output a fully resolved state (`the next node is X') or an abstract instruction (`go east')?

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/crossctxthr.png}
\caption[Cross-context activation patching and head redundancy analysis.]{\textbf{Cross-context activation patching and head redundancy analysis.} (A) Outcome distribution from 1,000 cross-context patching trials, where Layer 1 attention outputs from donor contexts were transplanted into recipient contexts with different directions. 97.4\% trials show universal directional transfer, 2.2\% reproduce donor context nodes, 0.4\% produce unexpected outputs. (B) Individual head performance in cross-context transfer when other heads are zeroed out. 6 of 12 Layer 1 heads achieve 60\%+ success independently (N=100 trials per head), indicating distributed rather than specialised directional processing.}
\label{fig:cross_context_head_redundancy}
\end{figure}

\subsubsection{Cross--Context Transfer Analysis}

To resolve this ambiguity, we conducted a cross-context patching experiment using completely independent random walks from different grids. We took the hidden state generated by a direction token in one context (e.g., the vector for EAST in `... ab EAST') and patched it into a completely different context (e.g., replacing the vector for SOUTH in `... yz SOUTH'). For each trial, we constructed pairs of prompts:

\begin{equation}
\begin{split}
    P_{donor} &= \text{``...} n_i \text{ DIRECTION}_1 \text{''} \rightarrow m_1 \\
    P_{recipient} &= \text{``...} n_j \text{ DIRECTION}_2 \text{''} \rightarrow m_2
\end{split}
\end{equation}

where $n_i, n_j$ are different nodes, $\text{DIRECTION}_1 \neq \text{DIRECTION}_2$ are different movement directions, and $m_1, m_2$ are their respective valid next nodes. We extracted the hidden state vector $h_{donor}$ from Layer 1's output at the DIRECTION token position in $P_{donor}$ and patched it into the same position in $P_{recipient}$:

\begin{equation}
    h_{recipient}^{patched} = \begin{cases}
        h_{donor} & \text{at DIRECTION token} \\
        h_{recipient} & \text{otherwise}
    \end{cases}
\end{equation}

The patched vectors successfully redirected predictions in 97.4\% of trials (N=1000). This suggests that Layer 1 computes a near-universal, transferable instruction (like a `go east' command) rather than a node-specific update (like `the result of going east from ab'). However, 2.2\% of trials produced the donor context's next node and 0.4\% produced unexpected outputs, indicating the representations contain both abstract and context-specific components. While the high success rate suggests Layer 1 often computes directional information that transfers across contexts, the mixed outcomes indicate the representation likely contains both abstract directional components and context-specific elements, rather than being purely universal instructions. Most likely, the model's internal components are not perfectly modular, reflecting the emergent nature of its capabilities.

\subsubsection{Head Redundancy Analysis}

Finally, a head redundancy analysis showed that 6 of 12 Layer 1 heads could independently drive 60\%+ cross-context transfer when isolated (all other heads zeroed out, patched over 1000 trials, see Figure \ref{fig:cross_context_head_redundancy}B). This indicates a distributed, partly redundant mechanism: rather than a single specialised `direction head', multiple heads encode overlapping variants of the same computation. Such redundancy likely reflects the model's overparametrisation (GPT-2 small has far more capacity than this task requires) so directional processing is spread across several heads instead of being compressed into a minimal circuit.

% \subsection{Attention Pattern Analysis}

% To understand how attention heads process spatial information, we visualised attention patterns for the final direction token in loop completion tasks. We examined how the 12 heads in Layer 1 allocate attention across the sequence, using the attention analysis approach detailed in Chapter 3.


% TODO: fix this

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{figures/l1attention.png}
% \caption[Layer 1 attention head patterns during loop completion task]{\textbf{Layer 1 attention head patterns during loop completion task}. Attention weights visualised for the final direction token across 12 heads. Different heads attend to different temporal positions: Heads 2, 5, 7, 10, 11, 12 focus on, TODO; finish this}
% \label{fig:layer1_attention}
% \end{figure}

% TODO: Add overparametrisation section/figure (maybe in appendix?)

\subsection{Testing the Three--Stage Hypothesis}

Having localised basic directional processing and observed structured spatial representations, we now test our hypothesis about the three--stage processing pipeline through targeted causal interventions.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/loopdirectionablation.png}
\caption[Stratified direction token ablation results]{\textbf{Stratified direction token ablation results reveal multiple computational strategies.} Historical direction tokens were zeroed out at each layer's input on 1,000 loop completion trials, stratified by loop length (2 to 12 hops). The results show two distinct patterns: \textbf{(1)} Trivial 2-hop loops are solved by Layer 2, suggesting a fast, local heuristic circuit in early layers. \textbf{(2)} Complex 4-12 hop loops show a consistent phase transition between Layers 6-8, with performance recovering significantly at Layer 7 and reaching near-perfection for all lengths by Layer 8. This indicates that the global coordinate system becomes fully self-sufficient by Layer 8, independent of task complexity.}
\label{fig:direction_ablation}
\end{figure}

\subsubsection{Direction Token Ablation}

Our representational analyses showed the model forms a stable coordinate system by Layer 7. However, these are correlational findings--they show information is present but not that it's causally used. To test when (if ever) the coordinate system becomes self--sufficient, we examine when the model no longer requires explicit directional information. We conduct a layer-wise direction token ablation, stratified by the complexity of the task. We use controlled loop completion tasks of varying lengths (2, 4, 6, 8, 10, and 12 hops), where a path returns to its starting node (e.g., `aa NORTH bb WEST cc SOUTH dd EAST'). The model's task is to predict the final node (aa) given the prompt. The intervention involves zeroing out the hidden states of all historical direction tokens at the input to each layer, one by one. The final direction token is always preserved. This design choice isolates the effect of historical memory from the effect of the immediate, necessary input. This approach, detailed in Chapter 3, tests whether spatial information has been `absorbed' into node representations.

The intervention reveals that the model employs at least two distinct strategies depending on task complexity (Figure \ref{fig:direction_ablation}):

\begin{enumerate}
\item \textbf{2-Hop Loops:} The simplest, 2-hop loops exhibit qualitatively different behaviour. Performance recovers to 65\% when ablating input to Layer 1 and reaches 100\% by Layer 2. A 2-hop loop is a simple `reverse the last move' pattern that does not require a global map (e.g., `xq EAST fc WEST'). Consistent with our reverse bias analysis, this suggests the model uses a specialised, low-level circuit in its earliest layers to solve these trivial local patterns without engaging its more complex spatial reasoning mechanisms.
\item \textbf{4-12 Hop Loops:} For all loops requiring more complex spatial reasoning (4+ hops), we observe a sharp, but slightly staggered, phase transition between Layers 6 and 8.
\begin{itemize}
    \item \textbf{Layers 1--6:} Ablating historical directions results in near-complete failure, confirming that the model relies on these explicit tokens to build its spatial representation in the early and middle layers.
    \item \textbf{Layer 7:} This marks the critical point of transition. Performance for all complex loops jumps dramatically.
    \item \textbf{Layer 8:} By Layer 8, performance for all complex loops, including the longest 12-hop variants, recovers to perfect accuracy. This is the point at which the internal coordinate system is fully self-sufficient and robust enough to solve even the most complex tasks without needing the original direction tokens from the path.
\end{itemize}
\end{enumerate}

The sharp transition at Layer 7 indicates that the spatial representation becomes self--sufficient--coordinate information has been fully absorbed into node states, making original direction tokens redundant. An important distinction emerges between our correlational and causal measurements. The apparent contradiction between the linear probing plateau at Layer 5 (R$^2$ $\approx 0.9$) and the direction ablation transition at Layer 7 reveals how the model processes spatial information. Linear probing shows whether coordinate information is linearly decodable in hidden states—a correlational measure. Direction ablation tests whether the model causally depends on explicit direction tokens—a causal measure. Between Layers 5--7, the model likely uses two sources of spatial information: position encoded in node hidden states (detectable by probes) and explicit direction tokens. This underscores how mechanistic interpretability requires complementary techniques: neither measure alone captures the full picture of spatial reasoning in the transformer.


% \subsection{Attention Pattern Analysis}

% To understand how attention heads process spatial information, we visualised attention patterns for the final direction token in loop completion tasks. We examined how the 12 heads in Layer 1 allocate attention across the sequence, using the attention analysis approach detailed in Chapter 3.


\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Latex template/figures/directionswap.png}
\caption[Direction swapping experiment results by node constraint type.]{\textbf{Direction swapping experiment results by node constraint type.} Cosine similarity between hidden states from original and geometrically mirrored paths (all directions reversed) on 500 trials per node type. Unconstrained centre nodes (4 available directions) show high similarity (cos$\approx$0.98), indicating path history discarded when irrelevant. Constrained corner nodes (2 available directions) show low similarity (cos$\approx$0.31), demonstrating selective preservation of path history when geometrically necessary.}
\label{fig:direction_swapping}
\end{figure}

% TODO: mention how dip at layer 8 corresponds to shift in organising principle--> pca variance explodes


\subsubsection{Direction Swapping}

Having established coordinate system formation by Layer 7, we test whether late layers simply preserve coordinates or refine them into functional representations. Based on our PCA results showing functional clustering, we hypothesise that the model learns not just \textit{where it is} but \textit{where it can go} from that position.

We systematically reverse all direction tokens in random walks (NORTH$\leftrightarrow$SOUTH, EAST$\leftrightarrow$WEST) and examine how representations respond to these geometrically mirrored paths. Note that swapping directions creates a new path that is a geometric mirror image of the original (sequence with identical node names and step counts but a completely different spatial trajectory). This allows us to test if the model's understanding is based on the geometric meaning of the path or just superficial token patterns. We compare responses at constrained versus unconstrained positions across 100 trials, focusing on how the model's behaviour varies based on geometric context: specifically comparing unconstrained centre positions (all four directions available), edge nodes (three available directions) and constrained corner nodes (only two directions available), using the intervention methodology from Chapter 3.

This experiment provides a causal explanation for the functional clustering observed in the late--layer PCA. The model's response to the mirrored path history depends entirely on the geometric context of the final node (Figure \ref{fig:direction_swapping}). At unconstrained positions (i.e. centre node), where the path taken is irrelevant to the four available moves, the hidden states for original and swapped sequences converge to be nearly identical (cosine similarity $\approx$0.99). This reveals the model learns to discard unnecessary historical details, which explains why all centre nodes collapsed into a single representation in the PCA. Conversely, at constrained corner nodes, where path history is critical for determining the two valid moves, the hidden states remained distinct (cosine similarity $\approx$0.31). Edge nodes show intermediate behaviour (cosine similarity $\approx$0.70), consistent with their intermediate geometric constraints (3 available moves).

This differential sensitivity emerges specifically during layers 8-10, coinciding with the shift from coordinate to affordance clustering observed in our PCA analysis. The synchronised timing of cosine similarity drops and representational reorganisation points to a computational phase transition. The model refines representations to emphasise functionally relevant distinctions while allowing irrelevant variations to wash out. Functionally equivalent states (centre node) naturally converge during this refinement process, while functionally distinct states (edges and corners) naturally diverge, producing the observed clustering by navigational affordances.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/4hoplastdir.png}
\caption[Layer 1 attention patterns for the final direction token.]{\textbf{Layer 1 attention patterns for the final direction token in a 4-hop loop.} A majority of heads (2, 5, 7, 9-12) focus their attention almost exclusively on the penultimate node token. Head 4 attends exclusively to node tokens, while Head 8 attends to direction tokens. Averaged over 1,000 trials with randomised node names; error bars = ±1 SD.}
\label{fig:layer1_attention_patterns}
\end{figure}

\subsection{Attention Pattern Analysis}

To explore the mechanisms that might underlie the model’s spatial reasoning, we analysed its attention patterns, focusing on the computation that occurs when predicting the final node in loop completion tasks. We visualised how attention heads allocate weights across sequence positions when processing the final direction token, across different loop lengths (2-12 hops). Across all loop complexities (2--12 hops) tested, we observe a consistent pattern in Layer 1 when querying the final direction token: approximately 6 heads almost exclusively focus attention on the penultimate node, i.e. the node token that appears before the final node in the sequence (Figure \ref{fig:layer1_attention_patterns}). Rather than processing static positional information, the model seems to encode trajectory by attending to a two-step history. This pattern holds regardless of task complexity, suggesting a universal low-level circuit for spatial perception which may form the foundation for all subsequent spatial reasoning.  

This simple Layer 1 circuit elegantly explains the model's behaviour on 2-hop loops observed in our direction ablation experiments (Figure \ref{fig:direction_ablation}). In a 2-hop loop (e.g., `aa EAST bb WEST'), the penultimate node is simply the origin (`aa'). By attending to it, Layer 1 provides a direct signal for the required reverse move. This creates a highly efficient heuristic that allows the model to solve these trivial cases in its earliest layers, bypassing the need for a global map. This explains why performance recovers almost immediately by Layer 2 even when the first direction token is ablated, and accounts for the high accuracy and strong reverse bias observed at minimal context lengths. Concurrently, other heads in Layer 1 exhibit a different kind of specialisation based on token type. We observe that certain heads, such as Head 4 and Head 8, selectively attend to either only node tokens or only direction tokens.

\begin{figure}[htbp]
\ContinuedFloat
\centering
\includegraphics[width=1\textwidth]{Latex template/figures/l104hoplastdir.png}
\caption[Layer 10 attention patterns for the final direction token.]{\textbf{Layer 10 attention patterns for the final direction token in a 4-hop loop.} Most heads strongly attend to the final direction token itself. Averaged over 1,000 trials with randomised node names; error bars = ±1 SD.}
\label{fig:layer10_attention_patterns}
\end{figure}


For longer, more complex loops, 2-step information alone is insufficient to find a solution. It may instead serve as the initial input to a deeper computational process that unfolds across the later layers of the network. While tracing this process in detail is beyond the scope of our current analysis, we can observe its potential outcome in the model's final layers. In the final layers (10–12), the computational focus shifts towards action selection, which is expected. This is most evident in the strong self-attention that multiple heads pay to the final direction token itself (Figure \ref{fig:layer10_attention_patterns}), likely preserving information for the final output. This specialisation aligns with our finding that late layers implement action-oriented representations.

Taken together, these observations suggest a coherent, though interpretive, mechanistic story. The model may not learn two entirely distinct circuits, but rather one foundational local rule that can be used in two different ways. For simple problems, this rule may be sufficient for a fast, heuristic solution. For more complex problems, it could serve as the building block for a more deliberative, multi-layer computation.


% \begin{figure}[htbp]
% \centering
%     % Panel (a): Layer 1
%     \begin{subfigure}{1\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Latex template/figures/4hoplastdir.png}
%         \caption{\textbf{Layer 1.} A majority of heads (here, Heads 2, 5, 7, 9, 10, 11, 12) focus their attention almost exclusively on the penultimate node token. Head 4 attends exclusively to node tokens, while head 8 attends to direction tokens.}
%         \label{fig:layer1_attention_patterns}
%     \end{subfigure}
    
%     % Panel (b): Layer 10
%     \begin{subfigure}{1\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Latex template/figures/l104hoplastdir.png}
%         \caption{\textbf{Layer 10.} Most heads strongly attend to the final direction token itself.}
%         \label{fig:layer10_attention_patterns}
%     \end{subfigure}

%     \caption[Attention patterns for the final direction token in a 4-hop loop.]{\textbf{Attention patterns for the final direction token in a 4-hop loop.} Comparison of Layer 1 (a) and Layer 10 (b). Results are averaged over 1,000 trials with randomised two-letter node names, and error bars indicate ±1 standard deviation across trials.}
%     \label{fig:attention_layers_1_vs_10}
% \end{figure}


% --- Layer 10 Panel, continued figure ---


\subsection{Summary }

The mechanistic interventions provide strong causal evidence for our hypothesised three--stage pipeline. We localised a direction--updating circuit in Layer 1 that computes universal movement instructions, demonstrated that by Layer 7 the model relies on a self--sufficient coordinate system, and revealed sophisticated context--dependent processing in the final layers. Having characterised the components of the learned algorithm, we can now assemble these findings to form a complete picture of how the Foraging Model solves its task.

\section{Chapter Discussion}

% The Foraging Model demonstrates that sophisticated spatial reasoning can emerge from minimal training objectives; a finding that challenges assumptions about the necessity of explicit goal-directed learning in spatial domains. The model's ability to achieve perfect accuracy on tasks it was never trained for (Hamiltonian cycle completion, geometric loop tasks) suggests that random walk prediction captures fundamental spatial constraints that generalise beyond the training distribution. fix this

\subsection{Three-Stage Processing Pipeline}

Our experiments provide initial evidence for how the Foraging Model may achieve spatial reasoning, though the precise computational mechanisms remain partly opaque. The model achieves impressive in-distribution and generalisation performance through what appears to be a three-stage process involving early directional processing, middle-layer spatial integration, and late-layer functional refinement. We emphasise that this `three-stage' description is a useful simplification of what is likely a more distributed and continuous process.\\

\textbf{Stage 1 - Directional Processing (Layer 1)}

Layer 1 attention implements the most interpretable component of the spatial reasoning pipeline. Cross-context patching transfers directional instructions successfully in 97.4\% of trials, providing strong evidence that this layer computes abstract movement operations that generalise across contexts. The high transfer rate suggests these representations capture something closer to universal directional concepts rather than position-specific updates. However, the 2.6\% failure rate (where patches produce unexpected outputs or donor context nodes) suggests the computation is not perfectly modular. These failures may reflect interference between abstract directional processing and context-specific information, or they may indicate that our experimental design incompletely isolates the relevant circuits. The distributed nature of this processing (across multiple redundant heads rather than a single specialised circuit) likely reflects the model's overparameterisation relative to task requirements.\\

\textbf{Stage 2 - Spatial Integration (Layers 2-7)}
Directional updates are progressively integrated into spatial representations. The emergence of coordinate-like organisation in middle layers is distinctive in PCA plots, with the first two principal components aligning surprisingly well with row and column structure. This pattern is consistent with the development of allocentric spatial representations, and shared structure across different experiences, which was also noted by Spens \& Burgess in their initial work \citep{Spens2024consolidation}. Linear probing shows gradual improvement in coordinate decodability (R² $\approx0.15$ at Layer 1 to R² $\approx0.93$ at Layer 8), confirming that coordinate information becomes linearly accessible. Critically, the direction ablation experiment provides causal evidence for when this coordinate system becomes functionally sufficient. Performance on complex loop completion tasks (4-12 hops) jumps from near-zero to perfect when historical direction tokens are removed at Layer 7 input, demonstrating that spatial relationships have been consolidated into node representations by this point. This suggests genuine computational restructuring rather than mere representational reorganisation.  We use the term `coordinate system' loosely here, since the model was not trained to map nodes to specific coordinates, and is more likely encoding relative positions and geometric constraints.\\

\textbf{Stage 3 - Functional Refinement (Layers 8-12)}

The map-like system observed in Layer 7 transforms into functional clustering based on navigational affordances. Corner nodes (2 available directions) form distinct clusters, edge nodes group by shared constraints, and centre nodes (4 available directions) converge into a single representation. The direction swapping experiment provides causal evidence: at constrained positions, representations remain distinct for geometrically different paths (cosine similarity $\approx 0.31$), while at unconstrained positions, they converge (cosine similarity $\approx 0.98$).\\


\subsection{Local Heuristics vs Global Planning}

Perhaps the most theoretically interesting finding is the model's adaptive use of different computational strategies based on available information. The U-shaped accuracy curve with respect to context length reveals how the model balances local heuristics against global reasoning. This pattern suggests the model employs qualitatively different computational approaches depending on available information. With short contexts (2-3 steps), the model achieves near-perfect performance with high reverse bias, indicating reliance on local heuristics. With minimal spatial information, the model defaults to reversing the previous move: a conservative strategy that avoids invalid moves in all grid configurations. In the intermediate context regime (5-40 steps), performance drops (70-85\% accuracy) while reverse bias decreases. This likely represents a transitional phase where local heuristics become less reliable but global spatial understanding remains incomplete. The model cannot yet build a complete spatial map but is exposed to enough spatial diversity to reduce reliance on simple reversal heuristics. Finally, as context length surpasses 40 steps, performance recovers to near-perfect levels ($\geq97\%$) with minimal reverse bias. With sufficient context, the model can maintain accurate spatial state and make informed predictions based on global spatial understanding rather than local heuristics. 

The loop-completion task provides convergent evidence for this local-to-global transition. The simplest 2-hop loops are solved almost immediately: performance recovers to 65\% by Layer 1 and reaches 100\% by Layer 2. These loops can be solved by a specialised, low-level circuit implementing the same reversal heuristic identified in the context-length analysis. Our attention pattern analysis provides a direct mechanistic explanation for this behaviour. We found that in Layer 1, a majority of attention heads consistently focus on the penultimate node token when processing a direction. For a simple 2-hop loop (e.g., aa EAST bb WEST), this circuit may implement the reversal heuristic: by attending to the origin node (aa), it provides both sufficient context and a direct signal for the target, solving the task in the earliest layers without needing a global map. More complex loops (4-12) hops, only succeed after a sharp transition around Layer 7, marking the point at which the internal coordinate system becomes fully self-sufficient. This adaptive strategy reveals a tension in next-token prediction for spatial tasks. Local heuristics provide robust performance when information is limited but become suboptimal as more spatial information becomes available. The model must `unlearn' reliable local strategies to benefit from global spatial reasoning—a process that temporarily reduces performance during the transition. This behaviour has implications beyond spatial reasoning. It suggests that transformers naturally develop hierarchical reasoning strategies that operate at different temporal scales, switching between them based on information availability. However, we observed this pattern in only one model on one task—broader generalisation claims require more evidence.

\subsection{The Challenge of `Hard' Decisions}

The Foraging Model reveals a fundamental asymmetry in navigation decision-making. When predicting the next node given a direction, the model achieves perfect accuracy (100\%). However, when required to predict both direction and node, performance drops to 98.3\%. This gap, while small, was found to be persistent, and reflects a deeper challenge than simply predicting more tokens. This asymmetry arises from the inherent nature of the decisions. As noted by \cite{bachmann2024ntp}, NTP tends to shift focus on local patterns and overlook `hard' decisions that require planning ahead. When predicting a node given a direction, there is exactly one correct answer determined by the grid structure: this is an `easy' decision with a deterministic outcome. In contrast, direction prediction often presents multiple valid options, requiring the model to look ahead and plan a path through the grid. The asymmetry is also observed in the model's generalisation to larger grids. Tasks with deterministic, geometrically constrained solutions (loop completion) maintain near-perfect performance, while open-ended tasks (random walk continuation) degrade more rapidly. This pattern also suggests that the model has genuinely learned abstract geometric principles rather than simply memorising statistical patterns from the training data.


\subsection{Does This Qualify as a `Cognitive Map'?}
% Note: 
The Foraging Model exhibits behaviours that superficially resemble elements of a cognitive map. Middle-layer representations can be linearly decoded into a Cartesian-like coordinate system, while late-layer states reflect functional clustering consistent with navigational affordances. These properties mirror spatial abstractions often attributed to hippocampal cognitive maps, but caution is warranted in making direct analogies. Classical cognitive maps, as proposed by Tolman, support allocentric, path-independent reasoning that enables flexible planning and inference; the Foraging Model exhibits some of these features \citep{tolman1948cognitive}. The emergence of a stable, orthogonal coordinate system around Layer 7 shows that position representations become largely independent of path history, while perfect performance on Hamiltonian cycle completion indicates the model maintains global spatial relationships beyond local pattern matching. The transition from coordinate tracking to functional clustering in late layers suggests that the model encodes not just `where it is' but `what it can do from here', which is reminiscent of how spatial information from the hippocampus is used by downstream regions like the prefrontal cortex (PFC) for planning, decision-making, and action selection \citep{yu2015hippocampal}. 

However, these analogies have limits. The model’s spatial reasoning is tightly coupled to the statistics of its training environment; generalisation is strongest for tasks with deterministic geometric constraints and degrades when multiple valid continuations exist, highlighting that it has learned abstract spatial reasoning but not a fully flexible, relational map of space. In sum, the Foraging Model develops structured, functionally useful spatial representations that capture key aspects of allocentric encoding and action-oriented reasoning, but these representations fall short of the full flexibility, goal-directed inference, and environmental adaptability characteristic of biological cognitive maps.

\subsection{Limitations}

Several important limitations should be considered when interpreting these results. All findings are based on GPT-2 small architecture and 4$\times$4 grids. The generalisability to larger models or more complex environments remains to be tested. Our experimental findings reveal a pattern of redundancy that provides important insights into the model's internal architecture. The head redundancy analysis showed that 6 of 12 Layer 1 heads can independently achieve 60\%+ cross-context transfer success, indicating that directional processing is distributed across multiple redundant circuits rather than compressed into a minimal, elegant solution. This redundancy is further evidenced by layer ablation experiments, which demonstrated that multiple layers can be removed without breaking the model's performance, suggesting the computation is spread across redundant pathways rather than concentrated in essential components. 

The distributed, redundant architecture likely reflects the model's substantial overparameterisation (124M parameters for what is essentially a 16-node navigation problem). Rather than converging on one minimal pipeline, the model appears to have distributed spatial processing across redundant circuits, a byproduct of having far more capacity than the task requires. While this excess capacity complicates mechanistic analysis by creating multiple overlapping circuits, it also provides important insights into how transformers actually work. Finally, while we identify the endpoints of spatial processing (Layer 1 updates and 2-step attention mechanism, Layer 7 coordinate stabilisation), the exact transformations occurring in intermediate layers remain largely unexplored. The sharp performance drop at training context length suggests fundamental working memory constraints that limit the model's spatial reasoning despite its robust cognitive map.

\subsection{Broader Implications}

The Foraging Model demonstrates that sophisticated spatial reasoning can emerge from simple training objectives. Passive exploration can produce more robust spatial representations than might be expected from next-token prediction alone. The three-stage processing pipeline reveals general principles for how local predictions can give rise to global understanding--through progressive abstraction and information integration across network layers. We see this at the most granular level with the simple Layer 1 attention pattern—a local, two-step computation that serves as the building block for both fast heuristics and, when integrated across layers, a robust global map. Even if the resulting implementation is less clean than a neatly hierarchical algorithm, the model clearly goes beyond simple pattern matching, integrating local predictions into coherent global understanding. These findings suggest a practical design principle: rather than hardcoding spatial inductive biases, we might design training objectives that naturally encourage the natural emergence of map-like internal representations. 

Our analysis of the Foraging Model establishes a clear baseline: passive exploration is enough to build a robust world model with a surprisingly interpretable structure. But this is only half the story. The critical question now is how this learned algorithm changes when the objective shifts from aimless wandering to active, goal-directed planning.  We investigate this in the next chapter by analysing the Shortest Path models.

